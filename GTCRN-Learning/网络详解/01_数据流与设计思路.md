# GTCRN 网络详解：数据流与设计思路

## 一、核心问题

语音降噪 = 从带噪语音中分离纯净语音。难点：噪声和语音在时域/频域高度混叠。

## 二、为什么用 U-Net

语音降噪是输入输出同尺寸的任务，U-Net 的跳跃连接能保留下采样丢失的细节：

```
编码器                           解码器
219频点 ────── skip1 ──────→ 219频点
    ↓ DSConv(stride=2)              ↑ DSDeconv
110频点 ────── skip2 ──────→ 110频点
    ↓ DSConv(stride=2)              ↑ DSDeconv
 55频点 ────── skip3~8 ─────→ 55频点
    ↓ GTConv×6                      ↑ GTConv×6
 55频点 ───────→ DPGRNN ×2 ────→ 55频点
```

---

## 三、原版 GTCRN 网络结构 (16kHz)

原版 GTCRN 来自论文，设计目标是超轻量 (23.67K 参数, 33.0 MMACs)。

### 3.1 整体数据流

```
输入 spec (B, 257, T, 2)  [16kHz, nfft=512]
    │
    │ 计算幅度: mag = sqrt(real² + imag²)
    │ 拼接: [mag, real, imag]  → (B, 3, T, 257)
    ▼
ERB.bm()
    │ 低频 65 bins 直接保留
    │ 高频 192 bins → nn.Linear → 64 ERB bands
    │ 输出: (B, 3, T, 129)
    ▼
SFE (Subband Feature Extraction)
    │ Unfold(1×3) 展开邻近频带
    │ 输出: (B, 9, T, 129)  [3通道×3邻频]
    ▼
Encoder
    │ ConvBlock(9→16, k=1×5, s=1×2): 129→65   → skip0
    │ ConvBlock(16→16, k=1×5, s=1×2, g=2): 65→33  → skip1
    │ GTConvBlock(16, d=1)                    → skip2
    │ GTConvBlock(16, d=2)                    → skip3
    │ GTConvBlock(16, d=5)                    → skip4
    │ 输出: (B, 16, T, 33)
    ▼
DPGRNN ×2
    │ GRNN: 分组GRU，输入分两半分别处理
    │ Intra: 双向 GRNN (频率轴)
    │ Inter: 单向 GRNN (时间轴)
    │ 输出: (B, 16, T, 33)
    ▼
Decoder
    │ GTConvBlock(16, d=5) + skip4
    │ GTConvBlock(16, d=2) + skip3
    │ GTConvBlock(16, d=1) + skip2
    │ DeconvBlock(16→16, k=1×5, s=1×2, g=2) + skip1: 33→65
    │ DeconvBlock(16→2, k=1×5, s=1×2) + skip0: 65→129
    │ 输出: (B, 2, T, 129)  [mask_r, mask_i]
    ▼
ERB.bs()
    │ 低频 65 直接保留
    │ 高频 64 ERB → nn.Linear → 192 bins
    │ 输出: (B, 2, T, 257)
    ▼
CRM (Complex Ratio Mask)
    │ out_r = spec_r × mask_r - spec_i × mask_i
    │ out_i = spec_r × mask_i + spec_i × mask_r
    ▼
输出 (B, 257, T, 2)
```

### 3.2 核心模块详解

**ERB 模块**
```
class ERB:
    erb_subband_1 = 65   # 低频直通数
    erb_subband_2 = 64   # 高频 ERB 数

    erb_fc = nn.Linear(257-65, 64)    # 高频压缩
    ierb_fc = nn.Linear(64, 257-65)   # 高频还原

    # 权重是预计算的三角滤波器组，但用 nn.Linear 存储
    # requires_grad=False，实际不训练

bm(): [低频65直通] + [高频192→64 ERB] = 129 频带
bs(): [低频65直通] + [64 ERB→高频192] = 257 频点
```

**SFE (Subband Feature Extraction)**
```
class SFE:
    unfold = nn.Unfold(kernel_size=(1,3))

    # 把每个频点和左右邻居拼起来
    # (B, 3, T, 129) → unfold → (B, 9, T, 129)
    # 相当于每个位置看 3 个频点的上下文
```

**TRA (Temporal Recurrent Attention)**
```
class TRA:
    att_gru = nn.GRU(C, C*2, 1)  # 时序建模
    att_fc = nn.Linear(C*2, C)   # 投影回原维度
    att_act = nn.Sigmoid()       # 门控

    forward(x):  # x: (B, C, T, F)
        zt = mean(x², dim=F)     # (B, C, T) 帧能量
        at = GRU(zt) → FC → Sigmoid  # (B, C, T) 门控
        return x * at.unsqueeze(-1)
```

**GTConvBlock (ShuffleNet 风格)**
```
class GTConvBlock:
    # 输入 x: (B, 16, T, F)
    # 分成两半: x1, x2 各 (B, 8, T, F)

    # 只处理 x1:
    x1 → SFE(k=3)           # (B, 8, T, F) → (B, 24, T, F)
       → PointConv(24→16)   # 压缩通道
       → DepthConv(16, 3×3, dilation)  # 时频卷积
       → PointConv(16→8)    # 恢复通道
       → TRA                # 时序门控

    # Channel Shuffle:
    shuffle(x1, x2)  # 交错拼接，促进信息交换

    # 输出: (B, 16, T, F)
```

**GRNN (Grouped RNN)**
```
class GRNN:
    # 把输入分成两组，分别过 GRU
    rnn1 = GRU(input//2, hidden//2)
    rnn2 = GRU(input//2, hidden//2)

    forward(x):  # x: (B, seq, input)
        x1, x2 = chunk(x, 2, dim=-1)
        y1 = rnn1(x1)
        y2 = rnn2(x2)
        return cat([y1, y2], dim=-1)
```

**DPGRNN (Dual-Path Grouped RNN)**
```
class DPGRNN:
    intra_rnn = GRNN(16, 8, bidirectional=True)  # 频率方向
    inter_rnn = GRNN(16, 16, bidirectional=False) # 时间方向

    forward(x):  # x: (B, 16, T, 33)
        # Intra: 每个时间帧内，沿频率轴处理
        x → (B*T, 33, 16) → GRNN → FC → LN → 残差

        # Inter: 每个频率bin，沿时间轴处理
        x → (B*33, T, 16) → GRNN → FC → LN → 残差

        return x  # (B, 16, T, 33)
```

### 3.3 原版网络参数

| 模块 | 结构 | 参数量 |
|------|------|--------|
| ERB | nn.Linear (固定权重) | ~25K (不计入) |
| SFE | Unfold | 0 |
| Encoder Conv×2 | Conv2d + BN | ~2K |
| GTConvBlock×3 | SFE+Conv+TRA | ~8K |
| DPGRNN×2 | GRNN×2 + FC + LN | ~12K |
| Decoder Conv×2 | ConvT2d + BN | ~2K |
| GTConvBlock×3 | 同编码器 | ~8K |
| **总计** | | **~23.67K** |

### 3.4 原版设计特点

1. **ShuffleNet 风格**: GTConvBlock 只处理一半通道，另一半直通，然后 shuffle
2. **分组 RNN**: GRNN 把通道分两组，各用独立 GRU，减少交互但省参数
3. **TRA 用 RNN**: 时序门控用 GRU 实现，有隐藏状态
4. **ERB 用 Linear**: 虽然权重固定，但用 nn.Linear 存储
5. **SFE 用 Unfold**: 直接展开邻频，通道数×3

---

## 四、从原版 GTCRN 到 V1：算子级轻量化

原版 GTCRN 是 16kHz，V1 做了 48kHz 适配 + 算子级优化。

**核心约束**：数据流不变，只替换算子。

### 4.1 改动对照

| 位置 | 原版 | V1 | 为什么 |
|------|------|-----|--------|
| ERB | nn.Linear (可训练) | register_buffer (固定) | ERB 是固定的心理声学变换，不需要学习。固定后不计参数，导出一致 |
| 卷积 | 标准 Conv2d | DW-Separable | 参数省 1/7 |
| TRA | RNN 门控 | TRALite (Conv) | 去状态，方便流式和量化 |
| DPGRNN | GRU(C, C) | C→r→C 瓶颈 | RNN 隐藏维是参数大头，低秩投影省参数 |
| 频带数 | 129 (16kHz) | 219 (48kHz) | 覆盖更宽频率 |
| GTConv层数 | 3 | 6 | 增强建模能力 |

### 4.2 DW-Separable 卷积

把标准卷积拆成 depthwise + pointwise：

```
标准 3×3 Conv (32→32):
    参数 = 3 × 3 × 32 × 32 = 9216

DW-Separable:
    Depthwise:  3 × 3 × 32 = 288      (每通道独立卷积)
    Pointwise:  1 × 1 × 32 × 32 = 1024 (混合通道)
    总参数 = 1312

省了约 1/7
```

结构：
```
输入 (B, C_in, T, F)
    ↓
Depthwise Conv2d(C_in, C_in, k, stride, groups=C_in)
    ↓
Pointwise Conv2d(C_in, C_out, 1×1)
    ↓
BatchNorm2d → PReLU
    ↓
输出 (B, C_out, T, F')
```

### 4.3 TRALite

原版 TRA 用 RNN 做时域门控，有隐藏状态，流式困难。

改成卷积，无状态：
```
输入 x (B, C, T, F)
    ↓
energy = mean(x², dim=F)        # (B, C, T) 每帧能量
    ↓
DW-Conv1d(C, C, k=5, groups=C)  # 时间轴卷积
    ↓
PW-Conv1d(C, C, k=1)            # 混合通道
    ↓
Sigmoid                          # 门控权重 (B, C, T)
    ↓
gate.unsqueeze(-1)              # (B, C, T, 1)
    ↓
output = x * gate
```

### 4.4 DPGRNN 瓶颈化

在 GRU 前后加低秩投影：
```
原版:
    GRU(input_size=C, hidden_size=C)
    参数 ∝ C²

瓶颈版:
    Linear(C → r) → GRU(r, r) → Linear(r → C)
    参数 ∝ r²

    r = 0.75C 时，参数约 0.56C²
```

### 4.5 ERB 固定化

ERB 三角滤波器组是固定的心理声学变换：
```
原版:
    self.erb_fc = nn.Linear(F_high, erb2)  # 计入参数，可能漂移

V1:
    W = self._compute_erb_filters()        # 预计算三角滤波器
    self.register_buffer("W_bm", W.t())    # 不计参数
    self.register_buffer("W_bs", W)

    # 变换
    x_erb = torch.matmul(x_high, self.W_bm)
    x_lin = torch.matmul(x_erb, self.W_bs)
```

---

## 五、V1/V2 完整网络结构

V1 和 V2 网络结构相同，只是损失函数不同。

```
输入 spec (B, 513, T, 2)  [实部, 虚部]
    │
    │ 计算幅度: mag = sqrt(real² + imag²)
    │ 拼接: [mag, real, imag]
    ▼
freq_weights (513,)  可学习频带权重
    │
    ▼
ERB_48k.bm()
    │ 低频 171 bins 直接保留
    │ 高频 342 bins → matmul(W_bm) → 48 ERB bands
    │ 输出: (B, 3, T, 219)
    ▼
SFE_Lite
    │ DWConv2d(3, 3, k=(1,5), groups=3)  频率轴局部聚合
    │ PWConv2d(3, 3, k=1)
    │ BatchNorm2d
    │ 输出: (B, 3, T, 219)
    ▼
┌─────────────────── Encoder ───────────────────┐
│                                               │
│  DSConv(3→32, stride=(1,2))                  │
│      DWConv(3,3,k=(1,3),s=(1,2),g=3)         │
│      PWConv(3,32,k=1)                         │
│      BN → PReLU                               │
│      输出: (B, 32, T, 110)         → skip1    │
│                                               │
│  DSConv(32→32, stride=(1,2))                 │
│      输出: (B, 32, T, 55)          → skip2    │
│                                               │
│  GTConvLite × 6                               │
│      dilation = [1, 2, 4, 8, 4, 2]           │
│      每层输出: (B, 32, T, 55)      → skip3~8  │
│                                               │
│  SubbandAttention                             │
│      energy = mean(x², dim=(C,T))  → (B, 55) │
│      weights = Sigmoid(FC(55→13→55))         │
│      output = x * weights                     │
│                                               │
└───────────────────────────────────────────────┘
    │ 输出: (B, 32, T, 55)
    ▼
┌─────────────────── DPGRNN ×2 ─────────────────┐
│                                               │
│  Intra-path (频率方向, 双向):                  │
│      reshape: (B,32,T,55) → (B*T, 55, 32)    │
│      pre:  Linear(32 → 32)                   │
│      GRU:  bidirectional=True, layers=2      │
│            input=32, hidden=32 → output=64   │
│      post: Linear(64 → 32)                   │
│      reshape back + 残差(α) + LayerNorm      │
│                                               │
│  Inter-path (时间方向, 单向):                  │
│      reshape: (B,32,T,55) → (B*55, T, 32)    │
│      pre:  Linear(32 → 32)  (共享权重)        │
│      GRU:  bidirectional=False, layers=2     │
│            input=32, hidden=32               │
│      post2: Linear(32 → 32)                  │
│      reshape back + 残差(β) + LayerNorm      │
│                                               │
│  α, β 初始化为 0.5，可学习                     │
│                                               │
└───────────────────────────────────────────────┘
    │ 输出: (B, 32, T, 55)
    ▼
┌─────────────────── Decoder ───────────────────┐
│                                               │
│  GTConvLite × 6                               │
│      dilation = [2, 4, 8, 4, 2, 1]  (逆序)   │
│      每层: x = GTConv(x + skip[-(i+1)])      │
│                                               │
│  DSDeconv(32→32)                             │
│      x = x + skip2                           │
│      ConvTranspose: 55 → 110                 │
│      输出: (B, 32, T, 110)                    │
│                                               │
│  DSDeconv(32→2)                              │
│      x = x + skip1                           │
│      ConvTranspose: 110 → 219                │
│      输出: (B, 2, T, 219)  [mask_r, mask_i]  │
│                                               │
└───────────────────────────────────────────────┘
    │
    ▼
ERB_48k.bs()
    │ 低频 171 直接保留
    │ 高频 48 ERB → matmul(W_bs) → 342 bins
    │ 输出: (B, 2, T, 513)
    ▼
ApplyMask (复数乘法)
    │ out_r = spec_r * mask_r - spec_i * mask_i
    │ out_i = spec_r * mask_i + spec_i * mask_r
    ▼
输出 (B, 513, T, 2)
```

### 各模块参数量

| 模块 | 结构 | 参数量 |
|------|------|--------|
| ERB | register_buffer | 0 |
| freq_weights | (513,) | 513 |
| SFE_Lite | DW(1×5) + PW + BN | ~50 |
| DSConv ×2 | DW + PW + BN | ~3K |
| GTConvLite ×12 | DW + PW + BN + TRA + SE | ~80K |
| SubbandAttn | FC(55→13→55) | ~1.5K |
| DPGRNN ×2 | Linear×3 + GRU×2 + LN | ~50K |
| DSDeconv ×2 | DW + PW + BN | ~3K |
| **总计** | | **~139K** |

---

## 六、GTConvLite 详细结构

```
输入 x (B, 32, T, 55)
    │
    ├─────────────────────────────────┐
    ▼                                 │ 残差连接
DWConv2d(32, 32, k=(3,3))            │
    │ padding = (dilation, 1)         │
    │ dilation = (d, 1)  只在时间轴    │
    │ groups = 32                     │
    ▼                                 │
PWConv2d(32, 32, k=(1,1))            │
    ▼                                 │
BatchNorm2d(32)                       │
    ▼                                 │
PReLU                                 │
    ▼                                 │
TRALite                               │
    │ energy = x.pow(2).mean(dim=3)   │
    │ gate = Sigmoid(PW(DW(energy)))  │
    │ output = x * gate.unsqueeze(-1) │
    ▼                                 │
SEBlock                               │
    │ w = x.mean(dim=(2,3))           │
    │ w = Sigmoid(FC2(ReLU(FC1(w))))  │
    │ output = x * w.unsqueeze(-1,-1) │
    ▼                                 │
    + ←───────────────────────────────┘
    │
    ▼
输出 (B, 32, T, 55)
```

**Dilation 设计 [1,2,4,8,4,2]**：

```
Layer 1 (d=1):  ███           感受野 3 帧
Layer 2 (d=2):  █ █ █         感受野 5 帧，累积 ~7
Layer 3 (d=4):  █   █   █     感受野 9 帧，累积 ~15
Layer 4 (d=8):  █       █       █  感受野 17 帧，累积 ~31
Layer 5 (d=4):  █   █   █     开始收缩
Layer 6 (d=2):  █ █ █         继续收缩

为什么先扩大后收缩？
- 纯递增 [1,2,4,8,16,32] 有"网格效应"：d=8 时只看 0,8,16 帧，中间跳过
- 金字塔结构：收缩阶段填补扩张阶段跳过的位置
```

---

## 七、DPGRNN 详细结构

```
输入 x (B, C=32, T, F=55)
    │
    ▼
═══════════════ Intra-path (频率方向) ═══════════════
    │
    │ reshape: (B, C, T, F) → (B*T, F, C)
    │          (B, 32, T, 55) → (B*T, 55, 32)
    ▼
pre: Linear(32 → 32, bias=False)
    ▼
intra_gru: GRU(input=32, hidden=32, layers=2, bidirectional=True)
    │ 输入:  (B*T, 55, 32)
    │ 输出:  (B*T, 55, 64)   双向所以 hidden×2
    ▼
post: Linear(64 → 32, bias=False)
    ▼
reshape: (B*T, 55, 32) → (B, 32, T, 55)
    ▼
y = x + α * h          α 初始化 0.5，可学习
    ▼
LayerNorm(32)          在最后一维归一化
    │
═══════════════ Inter-path (时间方向) ═══════════════
    │
    │ reshape: (B, C, T, F) → (B*F, T, C)
    │          (B, 32, T, 55) → (B*55, T, 32)
    ▼
pre: Linear(32 → 32)   与 intra 共享权重
    ▼
inter_gru: GRU(input=32, hidden=32, layers=2, bidirectional=False)
    │ 输入:  (B*55, T, 32)
    │ 输出:  (B*55, T, 32)   单向
    ▼
post2: Linear(32 → 32, bias=False)
    ▼
reshape: (B*55, T, 32) → (B, 32, T, 55)
    ▼
out = y + β * z        β 初始化 0.5，可学习
    ▼
LayerNorm(32)
    │
    ▼
输出 (B, 32, T, 55)
```

**为什么 Intra 双向、Inter 单向？**

- Intra (频率轴)：频率不涉及时间因果，低频信息对高频有帮助，反之亦然，双向充分利用上下文
- Inter (时间轴)：要支持实时处理，当前帧只能看过去，必须单向。V1/V2 的 inter 本来就是单向的

---

## 八、V1 → V2：换损失函数

### 问题

V1 用标准 SpecRIMAGLoss，对所有帧权重相同。但键盘敲击、鼠标点击等瞬态噪音处理不好。DNSMOS 是整段平均，掩盖了这个问题。

### 方案

不改网络，只改 loss。检测能量突变的帧，权重放大：

```
# 计算帧能量
energy[t] = mean(|spec[t]|²)

# 检测能量突变
energy_diff[t] = |energy[t] - energy[t-1]|
threshold = mean(energy) × 2.0
is_transient[t] = energy_diff[t] > threshold

# 分配权重
weight[t] = 5.0 if is_transient[t] else 1.0

# 加权损失
loss = Σ weight[t] × frame_loss[t]
```

### 结果

- DNSMOS 持平 (3.1474 → 3.147)
- 瞬态噪音主观改善
- 训练时间变长 (29 → 71 epochs)

### 为什么不改网络

能用 loss 解决就不动架构。改 loss 只影响训练，推理零开销。

---

## 九、V2 → V3：因果化改造

### 问题

V1/V2 是离线模型，处理第 t 帧要看整个序列。实时场景用不了。

```
非因果: 处理 t 帧要看 [0, ..., t, ..., T-1]  →  延迟 = 整段
因果:   处理 t 帧只看 [0, ..., t]            →  延迟 = 单帧 10ms
```

### 需要改造的模块

| 模块 | V1/V2 (非因果) | V3 (因果) | 原因 |
|------|---------------|-----------|------|
| GTConvLite DWConv | padding=(d,1) 对称 | F.pad((pad_t,0)) 只左边 | 不能看未来帧 |
| TRALite Conv1d | padding=k//2 对称 | F.pad(x,(k-1,0)) 只左边 | 不能看未来帧 |
| DPGRNN inter | 单向 GRU | 单向 GRU (不变) | 本来就是单向 |
| DPGRNN intra | 双向 GRU | 双向 GRU (不变) | 频率轴不涉及因果 |

### 因果卷积原理

```
非因果 (kernel=3, dilation=2):
    时间轴:  t-4  t-3  t-2  t-1   t   t+1  t+2
                        █         ●         █
    输出 t 依赖: t-2, t, t+2  ← 看到了未来!

因果 (kernel=3, dilation=2):
    时间轴:  t-4  t-3  t-2  t-1   t   t+1  t+2
             █         █         ●
    输出 t 依赖: t-4, t-2, t  ← 只看过去

实现: 左边 pad (k-1)×d = 4，右边 pad 0
```

### V3 完整网络结构

```
输入 spec (B, 513, T, 2)
    │
    ▼
ERB_48k.bm(): 513 → 219
    │
    ▼
in_conv: Conv2d(2→3, k=1×1, bias=False)
    │ V3 去掉了 mag 输入，只用 [real, imag]
    │ 1×1 卷积扩展到 3 通道
    ▼
┌───────────────── CausalEncoder ─────────────────┐
│                                                 │
│  CausalDSConv(3→32, stride=(1,2))              │
│      pad_t = k[0] - 1 = 0                       │
│      DWConv → BN → SiLU → PWConv → BN → SiLU   │
│      输出: (B, 32, T, 110)           → skip1    │
│                                                 │
│  CausalDSConv(32→32, stride=(1,2))             │
│      输出: (B, 32, T, 55)            → skip2    │
│                                                 │
│  CausalGTConvLite × 6                           │
│      dilation = [1, 2, 4, 8, 4, 2]             │
│      kernel = 5×5 (比 V1/V2 的 3×3 更大)        │
│      pad_t = (5-1) × dilation                   │
│      每层: F.pad(x, (0,0,pad_t,0)) → DWConv    │
│      激活: SiLU (不是 PReLU)                    │
│      输出: (B, 32, T, 55)            → skip3~8  │
│                                                 │
│  SubbandAttention                               │
│                                                 │
└─────────────────────────────────────────────────┘
    │
    ▼
┌───────────────── CausalDPGRNN ×2 ───────────────┐
│                                                 │
│  Intra-path: 双向 GRU (频率轴，不变)             │
│                                                 │
│  Inter-path: 单向 GRU (时间轴，本来就是单向)     │
│                                                 │
└─────────────────────────────────────────────────┘
    │
    ▼
┌───────────────── CausalDecoder ─────────────────┐
│                                                 │
│  CausalGTConvLite × 6                           │
│      dilation = [2, 4, 8, 4, 2, 1]             │
│      每层: x = x + skip[-(i+1)]                │
│            x = CausalGTConv(x)                 │
│                                                 │
│  Fuse: Conv2d(64→32, k=1×1)                    │
│      x = Fuse(cat(x, skip2))                   │
│                                                 │
│  CausalDSDeconv: 55 → 110                      │
│                                                 │
│  x = x + skip1                                 │
│                                                 │
│  CausalDSDeconv: 110 → 219, 通道 32→2          │
│                                                 │
└─────────────────────────────────────────────────┘
    │
    ▼
out_conv: Conv2d(2→2, k=1×1, bias=True)
    │
    ▼
ERB_48k.bs(): 219 → 513
    │
    ▼
ApplyMask (复数乘法)
    │
    ▼
输出 (B, 513, T, 2)
```

### V1/V2 vs V3 模块对比

| 模块 | V1/V2 | V3 | 变化原因 |
|------|-------|-----|----------|
| 输入处理 | [mag,real,imag] → SFE_Lite | [real,imag] → Conv1×1 | 简化，去掉 mag |
| GTConv kernel | 3×3 | 5×5 | 更大感受野补偿因果性损失 |
| GTConv padding | (d,1) 对称 | F.pad((pad_t,0)) | 因果化 |
| 激活函数 | PReLU | SiLU | 更平滑，训练更稳定 |
| BN 位置 | DW→PW→BN→Act | DW→BN→Act→PW→BN→Act | 更频繁归一化 |
| TRA padding | k//2 对称 | (k-1,0) 左边 | 因果化 |
| Decoder skip | x = GTConv(x + skip) | 同左 | 不变 |
| Fuse 层 | 无 | Conv(64→32) | 新增，融合 skip |
| 参数量 | 139K | 145K | +4% |

### 流式推理状态

V3 做流式推理需要维护帧间状态：

```
1. CausalGTConv 历史帧缓存
   12 层，每层缓存 (kernel-1) × dilation 帧
   dilation = [1,2,4,8,4,2, 2,4,8,4,2,1]
   最大: (5-1) × 8 = 32 帧

2. CausalTRA 历史能量
   12 层，每层缓存 kernel-1 = 4 帧

3. Inter-GRU hidden state
   2 个 DPGRNN × 2 层 GRU
   hidden: (num_layers=2, B×F, hidden_size=16)

4. Skip connection 缓存
   8 组 skip，需要对齐编码器和解码器时间戳
```

### 结果

| 指标 | V2 | V3 | 变化 |
|------|-----|-----|------|
| 参数量 | 139K | 145K | +4% |
| DNSMOS | 3.15 | 2.98 | -5% |
| 延迟 | 整段 | 10ms | 实时 |
| RTF | - | 0.21 | 4.7倍余量 |

掉 0.17 分是预期内的，因果模型看不到未来，信息量必然少。

---

## 十、复数掩码 (CRM)

### 传统 IRM 的问题

```
IRM (Ideal Ratio Mask):
    mask = |clean| / |noisy|    只处理幅度
    enhanced = mask × noisy_mag  相位用带噪的

问题: 相位也被噪声污染，相位误差导致听感不自然
```

### CRM 的优势

```
CRM (Complex Ratio Mask):
    网络输出: mask = (m_r, m_i)
    带噪频谱: spec = (s_r, s_i)

    复数乘法 (a+bi)(c+di) = (ac-bd) + (ad+bc)i:
    out_r = s_r × m_r - s_i × m_i
    out_i = s_r × m_i + s_i × m_r

优势: 同时修正幅度和相位
```

---

## 十一、版本对比总结

| 版本 | 网络结构 | 损失函数 | 参数量 | DNSMOS | 实时 | 适用场景 |
|------|----------|----------|--------|--------|------|----------|
| V1 | 基础版 | RI+Mag | 139K | 3.15 | × | 离线后期、播客 |
| V2 | 同V1 | 瞬态感知 | 139K | 3.15 | × | 办公录音、键盘噪音 |
| V3 | 因果化 | RI+Mag | 145K | 2.98 | √ 10ms | 实时通话、直播 |

---

## 十二、设计思路总结

1. **感知对齐**: ERB 模拟人耳，低频精细高频粗糙，计算资源投入到人耳敏感的地方
2. **参数效率**: DW-Separable 省 1/7，DPGRNN 瓶颈化省 ~44%，ERB 固定化归零
3. **感受野扩展**: 空洞卷积金字塔 [1,2,4,8,4,2]，先扩大后收缩避免网格效应
4. **多尺度注意力**: TRALite(时间) + SEBlock(通道) + SubbandAttn(频率)
5. **双路径建模**: Intra(频率依赖，谐波/共振峰) + Inter(时间依赖，音素/节奏)
6. **复数域处理**: CRM 同时修正幅度和相位，比只处理幅度效果好
7. **因果性预留**: Inter-GRU 本来就是单向，V3 只需改卷积 padding
