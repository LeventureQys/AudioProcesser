# GTCRN 网络详解

## 一、整体架构

GTCRN 是一个 U-Net 风格的编码器-解码器网络，用于语音降噪。核心思路：把频谱压缩到低维空间处理，再还原回去。

为什么用 U-Net？降噪是"输入输出尺寸相同"的任务，U-Net 的跳跃连接能保留细节，避免下采样丢失信息。

三个版本：
- V1：基线，标准损失函数
- V2：瞬态感知损失，针对键盘鼠标等突发噪音
- V3：因果化改造，支持实时流式处理

## 二、V1/V2 数据流

V1 和 V2 网络结构完全相同，只是损失函数不同。

### 2.1 输入处理

```
原始音频
    ↓
STFT (n_fft=1024, hop=480)
    ↓
复数频谱 (B, 513, T, 2)
    ↓
可学习频带权重 freq_weights (513,)
    ↓
计算幅度: mag = sqrt(real² + imag²)
    ↓
拼接: [mag, real, imag] → (B, 3, T, 513)
    ↓
ERB变换: 513 → 219
    ↓
(B, 3, T, 219)
```

**ERB 变换**

513 个线性频点压缩到 219 个感知频带：
- 低频 171 个频点直接保留（一一对应）
- 高频 342 个频点压缩到 48 个 ERB band

代码实现（`ERB_48k.bm()`）：
```python
def bm(self, x):
    x_low = x[..., :171]           # 低频直接保留
    x_high = x[..., 171:]          # 高频部分
    x_high_erb = torch.matmul(x_high, self.W_bm)  # 矩阵乘法压缩
    return torch.cat([x_low, x_high_erb], dim=-1)
```

W_bm 是预计算的三角滤波器组，形状 (342, 48)。每个 ERB band 对应一组权重，把多个线性频点加权求和。

**可学习频带权重**

初始化时对不同频段设置不同权重：
```python
def _init_freq_weights(self):
    self.freq_weights[:85] = 1.2    # 0-4kHz 语音核心频段
    self.freq_weights[85:171] = 1.0  # 4-8kHz
    self.freq_weights[171:] = 0.8    # 8-24kHz 高频
```

训练过程中这些权重会继续调整。

### 2.2 特征提取 (SFE_Lite)

```python
class SFE_Lite(nn.Module):
    def __init__(self, in_ch=3, out_ch=3):
        self.dw = nn.Conv2d(in_ch, in_ch, kernel_size=(1,5),
                           padding=(0,2), groups=in_ch, bias=False)
        self.pw = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)

    def forward(self, x):
        return self.bn(self.pw(self.dw(x)))
```

- DWConv: 1×5 卷积，沿频率轴提取局部特征
- PWConv: 1×1 卷积，混合通道
- 参数量约 50 个

### 2.3 编码器

```
输入 (B, 3, T, 219)
         ↓
DSConv (3→32, stride=2): 219 → 110    ← skip1
         ↓
DSConv (32→32, stride=2): 110 → 55    ← skip2
         ↓
GTConvLite × 6 (dilation: 1,2,4,8,4,2) ← skip3~8
         ↓
SubbandAttention
         ↓
输出 (B, 32, T, 55)
```

**DSConv（深度可分离卷积下采样）**

```python
class DSConv(nn.Module):
    def __init__(self, in_ch, out_ch, k=(1,3), s=(1,1), p=(0,1)):
        self.dw = nn.Conv2d(in_ch, in_ch, k, stride=s, padding=p,
                           groups=in_ch, bias=False)
        self.pw = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.PReLU()

    def forward(self, x):
        return self.act(self.bn(self.pw(self.dw(x))))
```

stride=(1,2) 只在频率轴下采样，时间轴保持不变。

**GTConvLite**

```python
class GTConvLite(nn.Module):
    def __init__(self, ch, dilation, use_se=True):
        # 深度卷积，dilation 只作用在时间轴
        self.ds = nn.Conv2d(ch, ch, kernel_size=(3,3),
                           padding=(dilation, 1), dilation=(dilation, 1),
                           groups=ch, bias=False)
        self.pw = nn.Conv2d(ch, ch, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(ch)
        self.act = nn.PReLU()
        self.tra = TRALite(ch)
        self.se = SEBlock(ch) if use_se else nn.Identity()

    def forward(self, x):
        h = self.act(self.bn(self.pw(self.ds(x))))
        h = self.tra(h)
        h = self.se(h)
        return h + x  # 残差连接
```

dilation 设计：[1,2,4,8,4,2]
- 感受野逐渐扩大再收缩
- dilation=8 时，3×3 卷积的时间感受野是 17 帧
- 先扩大再收缩能缓解"网格效应"（相邻像素没有信息交流）

**TRALite（时间递归注意力）**

```python
class TRALite(nn.Module):
    def __init__(self, channels, k=5):
        self.dw = nn.Conv1d(channels, channels, k, padding=k//2,
                           groups=channels, bias=True)
        self.pw = nn.Conv1d(channels, channels, 1, bias=True)
        self.act = nn.Sigmoid()

    def forward(self, x):  # x: (B, C, T, F)
        e = x.pow(2).mean(dim=3)  # 沿频率轴求能量 → (B, C, T)
        g = self.pw(self.dw(e))   # 时间轴卷积
        g = self.act(g).unsqueeze(-1)  # → (B, C, T, 1)
        return x * g
```

计算每个时间帧的能量，然后用卷积学习时间维度的注意力权重。

**SEBlock（通道注意力）**

```python
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=4):
        self.fc1 = nn.Conv2d(channels, channels // reduction, 1)
        self.fc2 = nn.Conv2d(channels // reduction, channels, 1)

    def forward(self, x):  # (B, C, T, F)
        w = x.mean(dim=(2, 3), keepdim=True)  # 全局平均池化 → (B, C, 1, 1)
        w = torch.sigmoid(self.fc2(F.relu(self.fc1(w))))
        return x * w
```

Squeeze-and-Excitation：学习每个通道的重要性。

**SubbandAttention（频带注意力）**

```python
class SubbandAttention(nn.Module):
    def __init__(self, freq_dim):  # freq_dim = 55
        self.fc = nn.Sequential(
            nn.Linear(freq_dim, freq_dim // 4),  # 55 → 13
            nn.ReLU(inplace=True),
            nn.Linear(freq_dim // 4, freq_dim),  # 13 → 55
            nn.Sigmoid()
        )

    def forward(self, x):  # (B, C, T, F)
        energy = x.pow(2).mean(dim=(1, 2))  # → (B, F)
        weights = self.fc(energy).unsqueeze(1).unsqueeze(2)  # → (B, 1, 1, F)
        return x * weights
```

根据各频带能量分布，学习频带级别的注意力。

### 2.4 DPGRNN（双路径 GRU）

这是 GTCRN 的核心模块。

```python
class DPGRNN_Enhanced(nn.Module):
    def __init__(self, c, r, num_layers=2, bidir_intra=True):
        self.pre = nn.Linear(c, r, bias=False)
        self.post = nn.Linear((2 if bidir_intra else 1) * r, c, bias=False)
        self.post2 = nn.Linear(r, c, bias=False)

        # 双层 GRU
        self.intra = nn.GRU(r, r, num_layers=num_layers, batch_first=True,
                           bidirectional=bidir_intra, dropout=0.1)
        self.inter = nn.GRU(r, r, num_layers=num_layers, batch_first=True,
                           bidirectional=False, dropout=0.1)

        self.ln1 = nn.LayerNorm(c, eps=1e-8)
        self.ln2 = nn.LayerNorm(c, eps=1e-8)

        # 可学习残差缩放
        self.alpha = nn.Parameter(torch.ones(1) * 0.5)
        self.beta = nn.Parameter(torch.ones(1) * 0.5)
```

**Intra-path（频率轴）**

```python
# 输入 x: (B, C, T, F) = (B, 32, T, 55)
h = x.permute(0, 2, 3, 1).reshape(B * T, F, C)  # → (B*T, 55, 32)
h = self.pre(h)                                  # → (B*T, 55, 32)
h, _ = self.intra(h)                             # 双向GRU → (B*T, 55, 64)
h = self.post(h).reshape(B, T, F, C).permute(0, 3, 1, 2)  # → (B, 32, T, 55)

y = (x + self.alpha * h).permute(0, 2, 3, 1)
y = self.ln1(y).permute(0, 3, 1, 2)
```

把每个时间帧独立处理，沿 55 个频带跑双向 GRU。捕捉频率维度的依赖（谐波结构等）。

**Inter-path（时间轴）**

```python
z = y.permute(0, 3, 2, 1).reshape(B * F, T, C)  # → (B*55, T, 32)
z = self.pre(z)                                  # 共享 pre 权重
z, _ = self.inter(z)                             # 单向GRU → (B*55, T, 32)
z = self.post2(z).reshape(B, F, T, C).permute(0, 3, 2, 1)  # → (B, 32, T, 55)

out = (y + self.beta * z).permute(0, 2, 3, 1)
out = self.ln2(out).permute(0, 3, 1, 2)
```

把每个频带独立处理，沿时间轴跑 GRU。捕捉时间维度的依赖（音素变化等）。

**为什么 inter 用单向 GRU？**

V1/V2 的 inter 实际上也是单向的（看代码 `bidirectional=False`）。这是为了后续 V3 因果化做准备，同时单向 GRU 在时间轴上已经足够。

**可学习残差缩放 α 和 β**

初始化为 0.5，让网络自己学习残差的权重。避免训练初期残差太大导致不稳定。

### 2.5 解码器

```python
class Decoder_Enhanced(nn.Module):
    def __init__(self, ch):
        self.gtconvs = nn.ModuleList([
            GTConvLite(ch, dilation=2, use_se=False),
            GTConvLite(ch, dilation=4, use_se=False),
            GTConvLite(ch, dilation=8, use_se=True),
            GTConvLite(ch, dilation=4, use_se=True),
            GTConvLite(ch, dilation=2, use_se=True),
            GTConvLite(ch, dilation=1, use_se=True),
        ])
        self.up1 = DSDeconv(ch, ch, ...)   # 55 → 110
        self.up2 = DSDeconv(ch, 2, ...)    # 110 → 219
        self.fuse = nn.Conv2d(ch * 2, ch, kernel_size=1, bias=False)

    def forward(self, x, enc_outs):
        # enc_outs: [down1, down2, gt1, gt2, gt3, gt4, gt5, gt6]
        for i, gtconv in enumerate(self.gtconvs):
            skip = enc_outs[-(i+1)]  # 逆序取 skip
            x = gtconv(x + skip)

        skip = enc_outs[1]  # down2
        x = self.up1(x + skip)

        skip = enc_outs[0]  # down1
        x = self.up2(x + skip)
        return x
```

dilation 顺序 [2,4,8,4,2,1] 与编码器 [1,2,4,8,4,2] 对称。

### 2.6 输出处理

```python
# 解码器输出 m_erb: (B, 2, T, 219)
m_lin = self.erb.bs(m_erb)  # ERB 逆变换 → (B, 2, T, 513)

# 复数掩码乘法
spec2 = spec.permute(0, 3, 2, 1)  # → (B, 2, T, 513)
out = self.mask(m_lin, spec2)

class ApplyMask(nn.Module):
    def forward(self, mask, spec):
        # mask: (B, 2, T, F), spec: (B, 2, T, F)
        # mask[:,0] = real part, mask[:,1] = imag part
        s_r = spec[:,0] * mask[:,0] - spec[:,1] * mask[:,1]
        s_i = spec[:,1] * mask[:,0] + spec[:,0] * mask[:,1]
        return torch.stack([s_r, s_i], dim=1)
```

CRM（Complex Ratio Mask）：复数乘法 (a+bi)(c+di) = (ac-bd) + (ad+bc)i

## 三、V1 → V2：瞬态感知损失

### 3.1 问题

V1 用标准 SpecRIMAGLoss，对所有帧一视同仁。但键盘敲击、鼠标点击这类突发噪音处理效果不好。

### 3.2 方案

不改网络，只改损失函数。加入瞬态检测：

```python
# 伪代码
energy = frame_energy(noisy)
energy_diff = |energy[t] - energy[t-1]|
transient = energy_diff > threshold * mean_energy

loss = sum(weight[t] * frame_loss[t])
weight[t] = 5.0 if transient[t] else 1.0
```

配置：
```yaml
Loss: TransientAwareLoss
  lambda_ri: 30.0
  lambda_mag: 70.0
  compress_factor: 0.3
  transient_weight: 5.0    # 瞬态帧权重
  energy_threshold: 2.0
  smooth_window: 3
```

### 3.3 效果

| 指标 | V1 | V2 |
|------|-----|-----|
| DNSMOS | 3.1474 | 3.147 |
| Best Epoch | 29 | 71 |
| 瞬态噪音 | 一般 | 明显改善 |

DNSMOS 持平，但瞬态噪音主观听感改善。代价是训练时间变长。

## 四、V2 → V3：因果化改造

### 4.1 目标

支持实时流式处理。当前帧的输出只能依赖当前和过去的输入。

### 4.2 需要改的模块

| 模块 | V2（非因果） | V3（因果） |
|------|-------------|-----------|
| GTConvLite | padding=(d,1) 对称 | 只 pad 左边 |
| TRALite | Conv1d padding=2 | F.pad(x,(4,0)) |
| DSConv | 无时间 padding | 无变化 |
| DPGRNN intra | 双向 GRU | 双向（频率轴不影响因果） |
| DPGRNN inter | 单向 GRU | 单向（本来就是） |

### 4.3 CausalGTConvLite

```python
class CausalGTConvLite(nn.Module):
    def __init__(self, ch, dilation=1, use_se=True):
        self.pad_t = (5 - 1) * dilation  # kernel=5
        self.dw = nn.Conv2d(ch, ch, (5, 5),
                           padding=(0, 2*dilation),  # 频率轴对称
                           dilation=(dilation, dilation),
                           groups=ch, bias=False)
        self.bn1 = nn.BatchNorm2d(ch)
        self.pw = nn.Conv2d(ch, ch, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(ch)
        self.act = nn.SiLU(inplace=True)  # PReLU → SiLU
        self.se = SEBlock(ch) if use_se else nn.Identity()

    def forward(self, x):
        res = x
        if self.pad_t > 0:
            x = F.pad(x, (0, 0, self.pad_t, 0))  # 只 pad 左边（过去）
        x = self.act(self.bn1(self.dw(x)))
        x = self.act(self.bn2(self.pw(x)))
        x = self.se(x)
        return x + res
```

关键变化：
- kernel 从 3×3 改成 5×5
- 时间轴 padding 从对称改成只 pad 左边
- 激活函数从 PReLU 改成 SiLU
- 加了中间 BN

### 4.4 CausalTRALite

```python
class CausalTRALite(nn.Module):
    def __init__(self, num_f, ratio=4, kernel_size=5):
        self.pad_left = kernel_size - 1  # = 4
        hidden = max(1, num_f // ratio)
        self.conv = nn.Sequential(
            nn.Conv1d(num_f, hidden, kernel_size, padding=0),
            nn.SiLU(inplace=True),
            nn.Conv1d(hidden, num_f, 1),
            nn.Sigmoid()
        )

    def forward(self, x):  # (B, C, T, F)
        energy = (x ** 2).mean(dim=1)  # (B, T, F)
        energy = energy.permute(0, 2, 1)  # (B, F, T)
        energy_padded = F.pad(energy, (self.pad_left, 0))  # 只 pad 左边
        attn = self.conv(energy_padded)
        attn = attn.permute(0, 2, 1).unsqueeze(1)
        return x * attn
```

### 4.5 CausalDSConv

```python
class CausalDSConv(nn.Module):
    def __init__(self, in_ch, out_ch, k=(1,3), s=(1,2), p=(0,1)):
        self.pad_t = k[0] - 1  # 时间轴因果 padding
        self.dw = nn.Conv2d(in_ch, in_ch, k, stride=s,
                           padding=(0, p[1]), groups=in_ch, bias=False)
        self.bn1 = nn.BatchNorm2d(in_ch)
        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.act = nn.SiLU(inplace=True)

    def forward(self, x):
        if self.pad_t > 0:
            x = F.pad(x, (0, 0, self.pad_t, 0))
        x = self.act(self.bn1(self.dw(x)))  # DW 后加 BN+Act
        x = self.act(self.bn2(self.pw(x)))  # PW 后加 BN+Act
        return x
```

结构变化：
- V2: DWConv → PWConv → BN → PReLU
- V3: DWConv → BN → SiLU → PWConv → BN → SiLU

### 4.6 CausalDPGRNN

```python
class CausalDPGRNN(nn.Module):
    def __init__(self, ch, num_layers=2, bidir_intra=True):
        r = ch // 2 if bidir_intra else ch

        self.intra = nn.GRU(r, r, num_layers=num_layers, batch_first=True,
                           bidirectional=bidir_intra, ...)  # 频率轴双向
        self.inter = nn.GRU(r, r, num_layers=num_layers, batch_first=True,
                           bidirectional=False, ...)  # 时间轴单向
```

频率轴的双向 GRU 保留，因为频率轴不涉及时间因果。

### 4.7 输入处理变化

V3 去掉了 SFE_Lite，改用简单的 1×1 卷积：

```python
# V1/V2
x3 = torch.stack([mag, real, imag], dim=1)  # (B, 3, T, 219)
x3 = self.erb.bm(x3)
x3 = self.sfe(x3)  # SFE_Lite

# V3
x = torch.stack([real_erb, imag_erb], dim=1)  # (B, 2, T, 219)
x = self.in_conv(x)  # Conv2d(2→3, k=1×1)
```

V3 不用幅度作为输入，只用实部和虚部。

### 4.8 效果对比

| 指标 | V2 | V3 |
|------|-----|-----|
| 参数量 | 139K | 145K |
| DNSMOS | 3.15 | 2.98 |
| 实时 | 否 | 是 |
| 延迟 | - | 10ms |
| RTF | - | 0.21 |

质量下降 5% 是因果化的代价。

### 4.9 流式推理状态

V3 做流式推理需要维护：

1. **GTConv 历史帧缓存**
   - 12 层，每层缓存 (kernel-1) × dilation 帧
   - dilation=[1,2,4,8,4,2,2,4,8,4,2,1]
   - 最大缓存 4×8=32 帧

2. **TRA 历史帧**
   - 12 层，每层 4 帧

3. **GRU hidden state**
   - 2 个 DPGRNN × 2 层 inter GRU
   - 每个 hidden: (num_layers, B×F, hidden_size)

4. **Skip connection**
   - 8 组，需要缓存编码器输出

## 五、设计思路

### 5.1 为什么用 ERB？

1. **计算效率**：513 → 219，减少 57%
2. **感知对齐**：低频保留细节，高频合并，符合人耳特性
3. **可逆**：线性变换，可以精确逆变换

### 5.2 为什么用深度可分离卷积？

普通 3×3 卷积：C_in × C_out × 9 参数
深度可分离：C_in × 9 + C_in × C_out 参数

C=32 时：
- 普通：9216 参数
- 深度可分离：1312 参数（1/7）

### 5.3 为什么用 DPGRNN？

1. **参数效率**：比 Transformer 小很多
2. **流式友好**：GRU 天然支持流式
3. **双路径设计**：分别处理频率和时间依赖

### 5.4 为什么用 CRM？

IRM 只处理幅度，相位用带噪信号的。
CRM 同时处理幅度和相位，效果更好。

### 5.5 dilation 设计

[1,2,4,8,4,2] 的"金字塔"结构：
- 感受野逐渐扩大：捕捉长距离依赖
- 然后收缩：缓解网格效应
- 编码器和解码器对称

### 5.6 跳跃连接

8 个 skip connection：
- 2 个来自下采样层
- 6 个来自 GTConvLite

保留下采样前的细节信息，解码器不用从头学习。

## 六、参数量统计

| 模块 | V1/V2 | V3 |
|------|-------|-----|
| ERB | 0 (buffer) | 0 |
| SFE / in_conv | ~50 | ~6 |
| DSConv×2 | ~3K | ~4K |
| GTConvLite×12 | ~80K | ~85K |
| SubbandAttn | ~1.5K | ~1.5K |
| DPGRNN×2 | ~50K | ~50K |
| DSDeconv×2 | ~3K | ~3K |
| freq_weights | 513 | 0 |
| out_conv | 0 | ~4 |
| **总计** | **139K** | **145K** |

## 七、选型建议

| 场景 | 推荐 | 原因 |
|------|------|------|
| 播客后期、视频配音 | V1 | 质量最好 |
| 办公环境录音 | V2 | 键盘鼠标噪声处理更好 |
| 实时通话、直播 | V3 | 必须低延迟 |
| 嵌入式部署 | V3 + C | 资源占用小 |
