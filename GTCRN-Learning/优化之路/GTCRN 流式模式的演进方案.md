# GTCRN 模型演进：从离线到实时

## 三个版本概览

这个项目有三个版本的GTCRN模型：

| 版本 | 特点 | 参数量 | DNSMOS |
|------|------|--------|--------|
| V1 | 离线处理，效果最好 | 139K | 3.147 |
| V2 | 加了瞬态感知损失 | 139K | 3.147 |
| V3 | 实时流式，10ms延迟 | 145K | 2.983 |

简单说：V1是baseline，V2针对键盘敲击这类突发噪声做了优化，V3改成了因果模型可以实时跑。

## 网络结构

整体是个U-Net风格的编码器-解码器结构：

```
输入频谱 (513 bins)
    ↓
ERB变换 (513 → 219)
    ↓
编码器 (下采样 + GTConv×6)
    ↓
DPGRNN ×2 (双路径GRU)
    ↓
解码器 (GTConv×6 + 上采样)
    ↓
ERB逆变换 (219 → 513)
    ↓
输出 (Complex Ratio Mask)
```

几个关键模块：

**ERB变换**：把513个线性频点压缩到219个ERB频带。低频保留更多细节，高频合并。这样既减少计算量，又符合人耳听觉特性。

**GTConvLite**：深度可分离卷积 + 时间注意力(TRA) + SE通道注意力 + 残差连接。用了不同的dilation rate [1,2,4,8,4,2]来扩大感受野。

**DPGRNN**：双路径GRU。Intra-path沿频率轴跑（双向），Inter-path沿时间轴跑（V1/V2双向，V3单向）。

## V1 → V2：瞬态感知损失

### 问题

V1在DNSMOS上分数不错，但实际听的时候发现：键盘敲击、鼠标点击这类突发噪声处理得不太干净，会有残留。

这其实挺好理解的：DNSMOS是对整段音频的平均评分，但人耳对突发噪声特别敏感。一段3秒的音频里，哪怕只有0.1秒的噪声没处理好，听感就会差很多。

### 解决方案

不改架构，只改损失函数。思路是：检测出哪些帧是"瞬态帧"（能量突变的帧），然后给这些帧更高的损失权重。

```python
# 瞬态检测
energy = torch.sum(spec_mag ** 2, dim=-1)
energy_diff = torch.abs(energy[:, 1:] - energy[:, :-1])
transient_mask = energy_diff > (2.0 * mean_energy)

# 损失加权
# 瞬态帧权重5.0，普通帧权重1.0
```

参数是试出来的：transient_weight=5.0, energy_threshold=2.0。权重太高会导致语音失真，阈值太高会漏检。

### 效果

DNSMOS基本没变（3.1474 → 3.147），但瞬态噪声处理明显好了。

DNSMOS没提升甚至略降是正常的：优化器把更多"精力"放在瞬态帧上，稳态帧的表现会稍微牺牲一点。但从听感上来说是值得的。

## V2 → V3：因果化改造

### 为什么要因果化

V1/V2是离线模型，处理的时候可以看到整段音频，包括"未来"的帧。这样效果好，但没法实时用。

实时通话对延迟很敏感，一般要求端到端150ms以内。留给降噪的预算也就10-20ms。V2的感受野有好几百毫秒，根本没法用。

因果模型只能看当前帧和历史帧，延迟可以做到单帧（10ms）。

### 改了什么

主要改三个地方：

**1. 卷积的padding**

```python
# V2：对称padding，左右各填充一半
pad = dilation * (kernel - 1) // 2

# V3：只在左边填充
pad = dilation * (kernel - 1)
```

**2. TRA模块**

```python
# V2：对称卷积
self.conv = nn.Conv1d(ch, ch, kernel_size=5, padding=2)

# V3：因果卷积，手动左填充
self.conv = nn.Conv1d(ch, ch, kernel_size=5, padding=0)
x = F.pad(x, (4, 0))  # 只填充左边
```

**3. DPGRNN的Inter-path**

```python
# V2：双向GRU
self.inter_gru = nn.GRU(..., bidirectional=True)

# V3：单向GRU
self.inter_gru = nn.GRU(..., bidirectional=False)
```

注意Intra-path（沿频率轴）还是双向的，因为频率轴不涉及时间因果性。

### 流式推理的状态管理

因果模型做流式推理需要维护帧间状态：
- GTConv的历史帧缓存（12层，不同dilation）
- TRA的历史均值
- GRU的hidden state
- 编码器到解码器的skip connection

### 因果性验证

怎么确认模型真的是因果的？改一下第25帧之后的输入，看前25帧的输出变不变：

```python
x1 = torch.randn(1, 50, 513)
x2 = x1.clone()
x2[:, 25:, :] = torch.randn(1, 25, 513)  # 改后25帧

y1 = model(x1)
y2 = model(x2)

# 前25帧应该完全一样
assert (y1[:, :25] - y2[:, :25]).abs().max() < 1e-6
# 后25帧应该不一样
assert (y1[:, 25:] - y2[:, 25:]).abs().max() > 0
```

### 性能代价

参数量从139K涨到145K。主要是因为单向GRU要达到双向GRU的建模能力，需要把hidden_size从48加到64。

DNSMOS从3.147掉到2.983，下降了5%左右。这是因果模型的固有限制：看不到未来信息，理论上限就比非因果模型低。

但换来了10ms的延迟和0.21的RTF（实时因子），可以跑实时了。

## 什么时候用哪个版本

| 场景 | 推荐 | 原因 |
|------|------|------|
| 播客后期、视频配音 | V1 | 质量最好，不在乎延迟 |
| 办公环境录音 | V2 | 键盘鼠标噪声处理更好 |
| 实时通话、直播 | V3 | 必须低延迟 |

## 一些经验

**改损失函数还是改架构？**

如果问题是局部的（比如特定类型噪声处理不好），优先改损失函数或数据。改架构的代价大：要重新验证、可能引入bug、推理时有额外开销。

V1→V2就是典型例子：瞬态噪声问题通过损失函数解决，零推理开销。

**因果化的性能损失能接受吗？**

语音增强领域，5-10%的DNSMOS下降一般可以接受，前提是没有明显的artifact（杂音、断续）。V3掉了5%，但听感上还行。

关键是要做主观测试，不能只看数字。有时候客观指标掉了，主观感受反而没那么差。

**什么时候停止优化？**

- 达到应用需求了
- 边际收益太小了
- 碰到理论上限了（比如因果模型的信息论限制）

V3的2.983已经够用了，再往上优化投入产出比不高。

## 文件结构

```
archived_models/
├── v1_baseline/
│   └── best_model_epoch29_score3.1474.tar
├── v2_transient/
│   ├── config.yaml
│   └── best_model_epoch71_score3.147.tar
└── v3_causal_stream/
    ├── models/gtcrn_light_v3_48k_causal_v2.py
    ├── checkpoints/best_model_epoch35_score2.983.tar
    └── C_Stream/  # C语言实现
```

V3还有完整的C语言实现，方便嵌入式部署。
