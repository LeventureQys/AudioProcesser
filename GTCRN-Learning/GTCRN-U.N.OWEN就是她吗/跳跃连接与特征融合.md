# 跳跃连接与特征融合

GTCRN是U-Net风格的编码器-解码器结构，跳跃连接是其中的关键设计。

## 为什么需要跳跃连接

编码器逐层提取越来越抽象的特征，到DPGRNN的时候，原始的细节信息已经丢得差不多了。解码器要从这些抽象特征重建细节，很难。

跳跃连接把编码器每一层的特征直接传给解码器对应层：

```
编码器                         解码器
GTConv1 ─────────────────────→ GTConv6
   ↓                              ↑
GTConv2 ─────────────────────→ GTConv5
   ↓                              ↑
GTConv3 ─────────────────────→ GTConv4
   ↓                              ↑
下采样                          上采样
   ↓                              ↑
GTConv4 ─────────────────────→ GTConv3
   ↓                              ↑
GTConv5 ─────────────────────→ GTConv2
   ↓                              ↑
GTConv6 ─────────────────────→ GTConv1
   ↓                              ↑
        DPGRNN ×2
```

## 不同层的特征

| 层级 | 特征类型 | 感受野 |
|------|---------|--------|
| 浅层 (GTConv1-2) | 局部细节、边缘 | 小 |
| 中层 (GTConv3-4) | 音素片段、局部模式 | 中 |
| 深层 (GTConv5-6) | 全局模式、语义 | 大 |

语音增强需要各个尺度的信息：
- 浅层特征保留语音的精细结构
- 深层特征帮助区分语音和噪声

跳跃连接让这些信息都能被利用。

## 融合方式

GTCRN用的是拼接+卷积的方式：

```python
class DecoderBlock(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        # 拼接后用1x1卷积降维
        self.fuse = nn.Conv2d(in_ch + skip_ch, out_ch, kernel_size=1)
        self.gtconv = GTConvLite(out_ch)

    def forward(self, x, skip):
        # x: 来自上一层解码器
        # skip: 来自对应编码器层
        fused = torch.cat([x, skip], dim=1)
        fused = self.fuse(fused)
        return self.gtconv(fused)
```

为什么用拼接不用相加？
- 拼接保留更完整的信息
- 1x1卷积让网络学习怎么融合
- 代价是参数量稍微增加

## 下采样和上采样

编码器中间有下采样，解码器有对应的上采样：

```python
# 编码器下采样：频率维度减半
self.downsample = nn.Conv2d(ch, ch, kernel_size=(1, 2), stride=(1, 2))

# 解码器上采样：频率维度翻倍
self.upsample = nn.ConvTranspose2d(ch, ch, kernel_size=(1, 2), stride=(1, 2))
```

下采样进一步减少计算量，上采样恢复分辨率。跳跃连接在上采样后做，所以尺寸是匹配的。

## V3的状态管理

V3做流式推理时，跳跃连接需要缓存编码器的输出：

```python
class StreamingGTCRN:
    def __init__(self):
        # 缓存编码器各层输出
        self.skip_buffers = [None] * 6

    def encode_frame(self, x):
        skips = []
        for i, layer in enumerate(self.encoder):
            x = layer(x)
            skips.append(x)
        self.skip_buffers = skips
        return x

    def decode_frame(self, x):
        for i, layer in enumerate(self.decoder):
            skip = self.skip_buffers[5 - i]  # 倒序取
            x = layer(x, skip)
        return x
```

因为是逐帧处理，每帧的编码器输出直接传给同一帧的解码器，不需要跨帧缓存。

## 梯度流动

跳跃连接还有个好处：改善梯度流动。

深层网络训练时，梯度要从输出层一路传回输入层，经过太多层容易消失。跳跃连接提供了"捷径"，梯度可以直接从解码器传到编码器对应层。

这也是为什么U-Net结构比普通的编码器-解码器更容易训练。
