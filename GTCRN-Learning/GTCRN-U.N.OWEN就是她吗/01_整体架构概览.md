# 01. GTCRN 整体架构概览

GTCRN是个U-Net风格的编码器-解码器结构，用于实时语音降噪。

## 简化数据流

```
输入频谱 (513 bins)
    ↓
ERB变换 (513 → 219)
    ↓
编码器 (下采样 + GTConv×6)
    ↓
DPGRNN ×2 (双路径GRU)
    ↓
解码器 (GTConv×6 + 上采样)
    ↓
ERB逆变换 (219 → 513)
    ↓
输出 (Complex Ratio Mask)
```

513个线性频点压缩到219个ERB频带，低频保留更多细节，高频合并。这样既减少计算量，又符合人耳听觉特性。

## 完整网络结构 (V1/V2)

```
输入 spec (B, 513, T, 2)
    │
    ├─ 可学习频带权重 (513,)
    │
    ▼
ERB_48k.bm(): 513 → 219 频带
    │   低频171直接保留，高频342压缩到48个ERB band
    │
    ▼
SFE_Lite: 特征提取
    │   DWConv(3, 3, k=1×5) → PWConv(3, 3) → BN
    │
    ▼
┌─ Encoder ─────────────────────────────────────┐
│                                               │
│  DSConv (3→32ch, stride=2): 219 → 110        │
│      DWConv → PWConv → BN → PReLU            │
│                                    ← skip1   │
│  DSConv (32→32ch, stride=2): 110 → 55        │
│                                    ← skip2   │
│                                               │
│  GTConvLite × 6 (dilation: 1,2,4,8,4,2)      │
│      每层: DWConv(3×3) → PWConv → BN → PReLU │
│            → TRALite → SEBlock → 残差        │
│                                    ← skip3-8 │
│                                               │
│  SubbandAttention: 频带加权                   │
│      energy → Linear(55,13) → ReLU           │
│            → Linear(13,55) → Sigmoid         │
└───────────────────────────────────────────────┘
    │
    ▼
DPGRNN_Enhanced × 2
    │   pre:  Linear(32 → 32)
    │   intra: 双向GRU×2层 (频率轴, 55步)
    │   post: Linear(64 → 32)
    │   inter: 单向GRU×2层 (时间轴, T步)
    │   post2: Linear(32 → 32)
    │   + LayerNorm + 可学习残差缩放(α,β)
    │
    ▼
┌─ Decoder ─────────────────────────────────────┐
│                                               │
│  GTConvLite × 6 (dilation: 2,4,8,4,2,1)      │
│      + skip connections (逆序)               │
│                                               │
│  DSDeconv (32→32ch): 55 → 110   + skip2      │
│  DSDeconv (32→2ch):  110 → 219  + skip1      │
│                                               │
└───────────────────────────────────────────────┘
    │
    ▼
ERB_48k.bs(): 219 → 513 频带
    │
    ▼
CRM掩码: out = spec * mask (复数乘法)
    │
    ▼
输出 (B, 513, T, 2)
```

## V3 因果版网络结构

```
输入 spec (B, 513, T, 2)
    │
    ▼
ERB_48k.bm(): 513 → 219 频带
    │
    ▼
in_conv: Conv2d(2 → 3, k=1×1)
    │
    ▼
┌─ CausalEncoder ───────────────────────────────┐
│                                               │
│  DSConv (3→32ch, stride=2): 219 → 110        │
│      DWConv → BN → SiLU → PWConv → BN → SiLU │
│                                    ← skip1   │
│  DSConv (32→32ch, stride=2): 110 → 55        │
│                                    ← skip2   │
│                                               │
│  CausalGTConvLite × 6 (dilation: 1,2,4,8,4,2)│
│      每层: 因果DWConv(5×5) → BN → SiLU       │
│            → PWConv → BN → SiLU              │
│            → CausalTRA → SE → 残差           │
│                                    ← skip3-8 │
│                                               │
│  SubbandAttention: 频带加权                   │
└───────────────────────────────────────────────┘
    │
    ▼
CausalDPGRNN × 2
    │   pre:  Linear(32 → 32)
    │   intra: 双向GRU×2层 (频率轴, 55步)  ← 双向OK
    │   post: Linear(64 → 32)
    │   inter: 单向GRU×2层 (时间轴, T步)   ← 必须单向!
    │   post2: Linear(32 → 32)
    │   + LayerNorm + 可学习残差缩放(α,β)
    │
    ▼
┌─ CausalDecoder ───────────────────────────────┐
│                                               │
│  CausalGTConvLite × 6 (dilation: 2,4,8,4,2,1)│
│      + skip connections (逆序)               │
│                                               │
│  Fuse: Conv2d(64→32, k=1×1) + skip2          │
│  DSDeconv (32→32ch): 55 → 110                │
│  DSDeconv (32→2ch):  110 → 219  + skip1      │
│                                               │
└───────────────────────────────────────────────┘
    │
    ▼
out_conv: Conv2d(2 → 2, k=1×1)
    │
    ▼
ERB_48k.bs(): 219 → 513 频带
    │
    ▼
CRM掩码: out = spec * mask (复数乘法)
    │
    ▼
输出 (B, 513, T, 2)
```

## 各模块参数量

| 模块 | 结构 | 参数量 |
|------|------|--------|
| ERB | 固定滤波器组 | 0 (buffer) |
| SFE | DW(1×5) + PW(1×1) + BN | ~50 |
| DSConv×2 | DW + PW + BN + PReLU | ~3K |
| GTConvLite×12 | DW(3×3) + PW + BN + TRA + SE | ~80K |
| SubbandAttn | Linear(55→13→55) | ~1.5K |
| DPGRNN×2 | GRU×4 + Linear×3 + LN | ~50K |
| DSDeconv×2 | DW + PW + BN + PReLU | ~3K |
| freq_weights | 可学习 | 513 |

## 核心模块

### GTConvLite

编码器和解码器各有6层GTConv，每层包含：
- 深度可分离卷积（减少参数）
- 时间注意力模块（TRA）
- SE通道注意力
- 残差连接

用了不同的dilation rate [1,2,4,8,4,2] 来扩大感受野。

```
输入 x (B, 32, T, 55)
    │
    ├─────────────────────────┐
    ▼                         │
DWConv2d(32, 32, k=3×3)      │  分组卷积，dilation沿时间轴
    │  padding=(dilation, 1)  │
    ▼                         │
PWConv2d(32, 32, k=1×1)      │
    │                         │
    ▼                         │
BN → PReLU                    │
    │                         │
    ▼                         │
TRALite: 时序注意力           │
    │  energy = x².mean(F)    │
    │  gate = σ(PW(DW(energy)))
    │  out = x * gate         │
    ▼                         │
SEBlock: 通道注意力           │
    │  w = σ(fc2(relu(fc1(   │
    │        x.mean(T,F)))))  │
    │  out = x * w            │
    ▼                         │
    + ←───────────────────────┘ 残差
    │
    ▼
输出 (B, 32, T, 55)
```

### DPGRNN（双路径GRU）

这是GTCRN的核心，有两个路径：

**Intra-path**：沿频率轴跑，双向GRU。处理频率维度的依赖关系，比如谐波结构。

**Inter-path**：沿时间轴跑。V1/V2是双向（离线），V3是单向（实时）。处理时间维度的依赖关系。

两个DPGRNN模块串联，增强时序建模能力。

```
输入 x (B, 32, T, 55)
    │
    ▼
reshape → (B*T, 55, 32)
    │
    ▼
pre: Linear(32 → 32)
    │
    ▼
intra: 双向GRU (沿频率轴)
    │   input_size=32, hidden=32, layers=2
    │   输出 (B*T, 55, 64)
    │
    ▼
post: Linear(64 → 32)
    │
    ▼
reshape → (B, 32, T, 55)
    │
    ├─ y = x + α*h
    ▼
LayerNorm
    │
    ▼
reshape → (B*55, T, 32)
    │
    ▼
pre: Linear(32 → 32)  (共享权重)
    │
    ▼
inter: 单向GRU (沿时间轴)
    │   input_size=32, hidden=32, layers=2
    │
    ▼
post2: Linear(32 → 32)
    │
    ▼
reshape → (B, 32, T, 55)
    │
    ├─ out = y + β*z
    ▼
LayerNorm
    │
    ▼
输出 (B, 32, T, 55)
```

### 输出层

输出是Complex Ratio Mask（复数比值掩码），同时处理幅度和相位。

---

## 版本演进

### 三个版本对比

| 版本 | 改动点 | 参数量 | DNSMOS | 实时 |
|------|--------|--------|--------|------|
| V1 baseline | 基线 | 139K | 3.15 | × |
| V2 transient | 换损失函数 | 139K | 3.15 | × |
| V3 causal | 因果化改造 | 145K | 2.98 | √ |

### V1 → V2: 换损失函数

**问题**：V1用的是标准 SpecRIMAGLoss，对所有帧一视同仁。但实际听感上，键盘敲击、鼠标点击这类突发噪音处理得不好。

**方案**：不改网络，只改损失函数。加了瞬态检测：

```python
# 检测能量突变
energy_diff = |energy[t] - energy[t-1]|
transient = energy_diff > threshold * mean_energy

# 瞬态帧损失放大5倍
loss = Σ weight[t] * frame_loss[t]
weight[t] = 5.0 if transient[t] else 1.0
```

**结果**：
- DNSMOS 基本持平 (3.1474 → 3.147)
- 瞬态噪音主观听感明显改善
- 训练时间变长 (29 → 71 epochs)

### V2 → V3: 因果化

**问题**：V1/V2 是离线模型，要看完整段音频才能处理。没法用在实时场景（通话、直播）。

**方案**：把所有"偷看未来"的操作改掉：

| 模块 | V2 (非因果) | V3 (因果) |
|------|-------------|-----------|
| GTConvLite | padding=(d,1) 对称 | pad_t=(k-1)*d 左边 |
| TRALite | Conv1d padding=2 | F.pad(x,(4,0)) |
| DPGRNN inter | 双向GRU | 单向GRU |

频率轴的操作不用改，因为频率轴不涉及时间因果。

**其他改动**：
- 激活函数: PReLU → SiLU
- DSConv: 加了中间BN，顺序调整
- 参数量: 139K → 145K (+4%)

**结果**：
- DNSMOS: 3.15 → 2.98 (-5%)
- 延迟: 10ms (单帧)
- RTF: 0.21 (还有4.7倍余量)

---

## 流式推理的状态管理

V3做流式推理需要维护帧间状态：
- GTConv的历史帧缓存（12层，不同dilation）
- TRA的历史均值（12层，每层4帧）
- GRU的hidden state（2×DPGRNN × 2层）
- 编码器到解码器的skip connection（8组）

---

## 选型建议

| 场景 | 推荐 | 原因 |
|------|------|------|
| 播客后期、视频配音 | V1 | 质量最好，不在乎延迟 |
| 办公环境录音 | V2 | 键盘鼠标噪声处理更好 |
| 实时通话、直播 | V3 | 必须低延迟 |
| 嵌入式部署 | V3 + C实现 | 资源占用小 |
