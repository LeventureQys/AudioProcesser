# GTCRN 整体架构概览

语音降噪的 U-Net：输入带噪频谱，输出复数掩码，乘回去得到干净频谱。

---

## 一、整体数据流

```
带噪语音
    ↓ STFT
频谱 (B, 513, T, 2)  [实部, 虚部]
    ↓
ERB 压缩: 513 → 219 频带
    ↓
编码器: 下采样 219→110→55，GTConv×6 提特征
    ↓
DPGRNN×2: 频率方向+时间方向双路径建模
    ↓
解码器: GTConv×6 + 上采样 55→110→219
    ↓
ERB 还原: 219 → 513
    ↓
复数掩码 × 输入频谱
    ↓
干净频谱 → iSTFT → 干净语音
```

为什么用 ERB？人耳对低频敏感、高频迟钝。ERB 把 513 个线性频点压到 219 个感知频带：低频保留细节，高频合并。省计算，符合听感。

---

## 二、为什么是 U-Net

语音降噪是输入输出同尺寸的任务。U-Net 的跳跃连接能把编码器丢失的细节补回来：

```
Encoder                              Decoder
219 ─────────── skip1 ──────────→ 219
  ↓ stride=2                          ↑
110 ─────────── skip2 ──────────→ 110
  ↓ stride=2                          ↑
 55 ─────────── skip3~8 ─────────→ 55
  ↓ GTConv×6                          ↑ GTConv×6
 55 ──────→ DPGRNN×2 ──────→ 55
```

---

## 三、原版 GTCRN 网络结构 (16kHz)

原版 GTCRN 来自论文，设计目标是超轻量 (23.67K 参数, 33.0 MMACs)。

### 整体数据流

```
输入 spec (B, 257, T, 2)  [16kHz, nfft=512]
    │
    │ 拼接: [mag, real, imag] → (B, 3, T, 257)
    ▼
ERB.bm()
    │ 低频 65 bins 直通
    │ 高频 192 bins → nn.Linear → 64 ERB
    │ 输出: (B, 3, T, 129)
    ▼
SFE: Unfold(1×3)
    │ 展开邻近频带
    │ 输出: (B, 9, T, 129)
    ▼
Encoder
    │ Conv(9→16, k=1×5, s=1×2): 129→65     → skip0
    │ Conv(16→16, k=1×5, s=1×2, g=2): 65→33 → skip1
    │ GTConvBlock(16, d=1)                   → skip2
    │ GTConvBlock(16, d=2)                   → skip3
    │ GTConvBlock(16, d=5)                   → skip4
    ▼
DPGRNN ×2
    │ Intra: 双向 GRNN (频率轴)
    │ Inter: 单向 GRNN (时间轴)
    ▼
Decoder
    │ GTConvBlock + skip4, skip3, skip2
    │ Deconv: 33→65 + skip1
    │ Deconv: 65→129 + skip0
    │ 输出: (B, 2, T, 129)
    ▼
ERB.bs(): 129 → 257
    ▼
CRM 复数掩码
    ▼
输出 (B, 257, T, 2)
```

### 原版核心模块

**SFE (Subband Feature Extraction)**
```
unfold = nn.Unfold(kernel_size=(1,3))
# 把每个频点和左右邻居拼起来
# (B, 3, T, 129) → (B, 9, T, 129)
```

**TRA (Temporal Recurrent Attention)**
```
att_gru = nn.GRU(C, C*2, 1)  # 时序建模
att_fc = nn.Linear(C*2, C)
att_act = nn.Sigmoid()

forward(x):  # (B, C, T, F)
    zt = mean(x², dim=F)     # (B, C, T) 帧能量
    at = GRU(zt) → FC → Sigmoid
    return x * at.unsqueeze(-1)
```

**GTConvBlock (ShuffleNet 风格)**
```
输入 x: (B, 16, T, F)
分成两半: x1, x2 各 (B, 8, T, F)

只处理 x1:
  x1 → SFE(k=3)           # → (B, 24, T, F)
     → PointConv(24→16)
     → DepthConv(3×3, dilation)
     → PointConv(16→8)
     → TRA               # 时序门控

shuffle(x1, x2)  # 交错拼接
输出: (B, 16, T, F)
```

**GRNN (Grouped RNN)**
```
# 把输入分成两组，分别过 GRU
rnn1 = GRU(input//2, hidden//2)
rnn2 = GRU(input//2, hidden//2)

forward(x):
    x1, x2 = chunk(x, 2, dim=-1)
    y1, y2 = rnn1(x1), rnn2(x2)
    return cat([y1, y2], dim=-1)
```

**DPGRNN**
```
intra_rnn = GRNN(16, 8, bidirectional=True)
inter_rnn = GRNN(16, 16, bidirectional=False)

forward(x):  # (B, 16, T, 33)
    # Intra: 沿频率轴
    x → (B*T, 33, 16) → GRNN → FC → LN → 残差

    # Inter: 沿时间轴
    x → (B*33, T, 16) → GRNN → FC → LN → 残差
```

### 原版参数量

| 模块 | 参数量 |
|------|--------|
| ERB | ~25K (不计入) |
| Encoder Conv×2 | ~2K |
| GTConvBlock×6 | ~16K |
| DPGRNN×2 | ~12K |
| Decoder Conv×2 | ~2K |
| **总计** | **~23.67K** |

### 原版设计特点

1. **ShuffleNet 风格**: GTConvBlock 只处理一半通道，另一半直通后 shuffle
2. **分组 RNN**: GRNN 把通道分两组，各用独立 GRU
3. **TRA 用 RNN**: 时序门控用 GRU，有隐藏状态
4. **ERB 用 Linear**: 虽然权重固定，但用 nn.Linear 存储
5. **SFE 用 Unfold**: 直接展开邻频，通道数×3

---

## 四、原版 → V1：算子级轻量化

原版 GTCRN 是 16kHz 的，V1 适配 48kHz，同时做了四个算子级优化。

核心原则：数据流不动，只换算子。

### 改动一览

| 位置 | 原版 | V1 | 原因 |
|------|------|-----|------|
| ERB | nn.Linear | register_buffer | 固定变换不需要学，省参数 |
| 卷积 | Conv2d | DW-Separable | 参数省 1/7 |
| TRA | RNN 门控 | TRALite (Conv) | 无状态，好部署 |
| DPGRNN | GRU(C,C) | C→r→C | 低秩省参数 |
| 频带 | 129 | 219 | 48kHz 需要更多频带 |

### DW-Separable 原理

标准卷积一步到位，参数多：
```
3×3 Conv (32→32): 3×3×32×32 = 9216 参数
```

拆成两步：
```
Depthwise: 3×3×32 = 288      每通道独立卷积
Pointwise: 1×1×32×32 = 1024  混合通道
总计: 1312 参数，省了 ~7 倍
```

### TRALite 原理

原版 TRA 用 RNN 做时域门控，有隐藏状态。改成卷积：
```
x (B,C,T,F)
  ↓ mean(dim=F)
能量 (B,C,T)
  ↓ DW-Conv1d(k=5)
  ↓ PW-Conv1d
  ↓ Sigmoid
门控 (B,C,T,1) × x → 输出
```
无状态，可量化。

### DPGRNN 瓶颈化

RNN 参数 ∝ hidden²，加低秩投影：
```
原版: GRU(32, 32)           参数 ∝ 32²
瓶颈: Linear(32→24) → GRU(24,24) → Linear(24→32)
                            参数 ∝ 24² ≈ 0.56×32²
```

---

## 五、V1/V2 完整网络

V1 和 V2 结构相同，只是损失函数不同。

```
输入 (B, 513, T, 2) [real, imag]
  │
  │ mag = sqrt(real² + imag²)
  │ 拼接 [mag, real, imag]
  ▼
freq_weights (513,) 可学习
  │
  ▼
ERB.bm(): 低频171直通 + 高频342→48 ERB
  │       输出 (B, 3, T, 219)
  ▼
SFE_Lite: DW(1×5) + PW + BN
  │       频率轴局部聚合
  ▼
┌─────────── Encoder ───────────┐
│ DSConv(3→32, s=2): 219→110   │← skip1
│ DSConv(32→32, s=2): 110→55   │← skip2
│ GTConvLite ×6                 │← skip3~8
│   dilation: [1,2,4,8,4,2]    │
│ SubbandAttention              │
└───────────────────────────────┘
  │ (B, 32, T, 55)
  ▼
┌─────────── DPGRNN ×2 ─────────┐
│ Intra: (B*T,55,32)           │
│   → Linear → BiGRU → Linear  │
│   → reshape + 残差α + LN     │
│                               │
│ Inter: (B*55,T,32)           │
│   → Linear → GRU → Linear    │
│   → reshape + 残差β + LN     │
└───────────────────────────────┘
  │ (B, 32, T, 55)
  ▼
┌─────────── Decoder ───────────┐
│ GTConvLite ×6 + skip          │
│   dilation: [2,4,8,4,2,1]    │
│ DSDeconv: 55→110 + skip2      │
│ DSDeconv: 110→219 + skip1     │
└───────────────────────────────┘
  │ (B, 2, T, 219)
  ▼
ERB.bs(): 219 → 513
  │
  ▼
CRM: out = spec_r×mask_r - spec_i×mask_i
     out_i = spec_r×mask_i + spec_i×mask_r
  │
  ▼
输出 (B, 513, T, 2)
```

### GTConvLite 结构

```
x (B, 32, T, 55)
  │
  ├──────────────────┐ 残差
  ▼                  │
DWConv(3×3, dilation)│
  ↓                  │
PWConv(1×1)          │
  ↓                  │
BN → PReLU           │
  ↓                  │
TRALite (时域门控)    │
  ↓                  │
SEBlock (通道门控)    │
  ↓                  │
  + ←────────────────┘
  ↓
输出 (B, 32, T, 55)
```

Dilation [1,2,4,8,4,2] 先扩大后收缩：
- 扩大：感受野指数增长
- 收缩：填补扩张阶段跳过的位置，避免网格效应

### DPGRNN 结构

```
x (B, 32, T, 55)
  │
  │ Intra (频率方向, 双向)
  │ reshape → (B*T, 55, 32)
  ↓
Linear(32→32) → BiGRU(32,32) → Linear(64→32)
  │ reshape → (B, 32, T, 55)
  │ + α×residual + LayerNorm
  │
  │ Inter (时间方向, 单向)
  │ reshape → (B*55, T, 32)
  ↓
Linear(32→32) → GRU(32,32) → Linear(32→32)
  │ reshape → (B, 32, T, 55)
  │ + β×residual + LayerNorm
  ↓
输出 (B, 32, T, 55)
```

为什么 Intra 双向、Inter 单向？
- Intra 沿频率轴：低频和高频互相有帮助，双向更好
- Inter 沿时间轴：要支持实时，只能看过去

---

## 六、V1 → V2：换损失函数

### 问题

标准 loss 对每帧权重相同，但键盘/鼠标这类瞬态噪音帧很少，loss 贡献小，学不好。

### 方案

不改网络，只改 loss：
```
energy_diff[t] = |energy[t] - energy[t-1]|
is_transient[t] = energy_diff[t] > 2 × mean_energy

weight[t] = 5.0 if transient else 1.0
loss = Σ weight[t] × frame_loss[t]
```

### 结果

- DNSMOS 持平 (3.15)
- 瞬态噪音主观改善
- 训练变长 (29→71 epochs)

能用 loss 解决就不动网络，推理零开销。

---

## 七、V2 → V3：因果化

### 问题

V1/V2 处理第 t 帧要看整段，实时用不了。

```
非因果: 处理 t 需要 [0...t...T-1]  延迟=整段
因果:   处理 t 只需要 [0...t]      延迟=10ms
```

### 需要改的地方

| 模块 | V2 | V3 |
|------|-----|-----|
| Conv padding | (d,d) 对称 | ((k-1)×d, 0) 只左边 |
| TRA padding | k//2 | F.pad(x, (k-1, 0)) |
| GTConv kernel | 3×3 | 5×5 |
| 激活 | PReLU | SiLU |
| DPGRNN inter | 单向 | 单向 (不变) |
| DPGRNN intra | 双向 | 双向 (不变，频率轴无因果) |

Kernel 变大是为了补偿因果化损失的感受野。

### 因果卷积原理

```
非因果 (k=3, d=2):
  t-4  t-3  t-2  t-1   t   t+1  t+2
            █         ●         █
  看了 t+2，不行

因果 (k=3, d=2):
  t-4  t-3  t-2  t-1   t   t+1  t+2
   █         █         ●
  只看 t-4, t-2, t，可以

实现: 左边 pad (k-1)×d，右边 pad 0
```

### V3 完整网络

```
输入 (B, 513, T, 2)
  │
  ▼
ERB.bm(): 513 → 219
  │
  ▼
in_conv: Conv(2→3, 1×1)  去掉了 mag
  │
  ▼
┌────── CausalEncoder ──────┐
│ CausalDSConv: 219→110    │← skip1
│ CausalDSConv: 110→55     │← skip2
│ CausalGTConv ×6          │← skip3~8
│   F.pad((pad_t,0)) + DW  │
│   SiLU 代替 PReLU        │
│ SubbandAttention          │
└───────────────────────────┘
  │
  ▼
┌────── CausalDPGRNN ×2 ────┐
│ Intra: 双向 (频率轴OK)    │
│ Inter: 单向 (本来就是)    │
└───────────────────────────┘
  │
  ▼
┌────── CausalDecoder ──────┐
│ CausalGTConv ×6 + skip    │
│ Fuse(64→32) + DSDeconv    │
│ DSDeconv: 110→219 + skip1 │
└───────────────────────────┘
  │
  ▼
out_conv: Conv(2→2, 1×1)
  │
  ▼
ERB.bs(): 219 → 513
  │
  ▼
CRM
  │
  ▼
输出 (B, 513, T, 2)
```

### 流式推理状态

实时跑需要维护：
- GTConv 历史帧: 12层，每层最多 (k-1)×d = 32 帧
- TRA 历史能量: 12层，每层 k-1 = 4 帧
- Inter GRU hidden: 2×DPGRNN × 2层
- Skip 缓存: 8 组

---

## 八、参数量

| 模块 | V1/V2 | V3 |
|------|-------|-----|
| ERB | 0 | 0 |
| SFE/in_conv | ~50 | ~20 |
| DSConv×2 | ~3K | ~3K |
| GTConvLite×12 | ~80K | ~85K |
| SubbandAttn | ~1.5K | ~1.5K |
| DPGRNN×2 | ~50K | ~50K |
| DSDeconv×2 | ~3K | ~4K |
| **总计** | **~139K** | **~145K** |

V3 参数略多是因为 kernel 从 3×3 变成 5×5。

---

## 九、版本对比

| 版本 | 网络 | Loss | 参数 | DNSMOS | 延迟 | 场景 |
|------|------|------|------|--------|------|------|
| V1 | 基线 | RI+Mag | 139K | 3.15 | 整段 | 离线后期 |
| V2 | 同V1 | 瞬态感知 | 139K | 3.15 | 整段 | 办公录音 |
| V3 | 因果 | RI+Mag | 145K | 2.98 | 10ms | 实时通话 |

V3 掉 0.17 分是预期的：因果模型看不到未来，信息量少。

---

## 十、设计要点

1. **ERB 对齐感知**: 低频精细高频粗糙，符合人耳
2. **DW-Separable**: 空间卷积和通道混合分开，省参数
3. **双路径 RNN**: Intra 建模频率依赖，Inter 建模时间依赖
4. **Dilation 金字塔**: 先扩大后收缩，避免网格效应
5. **多尺度注意力**: TRA(时间) + SE(通道) + Subband(频率)
6. **复数掩码**: 同时修正幅度和相位
7. **因果预留**: Inter 本来就是单向，V3 只需改 padding

---

## 附录

详细的背景知识请参考附录文件夹：

- [附录 A：U-Net 详解](./附录/A_U-Net详解.md) - U-Net 的原理、跳跃连接、在 GTCRN 中的应用
- [附录 B：卷积参数速查](./附录/B_卷积参数速查.md) - k/s/p/d/g 参数含义、尺寸计算公式
- [附录 C：GTConvBlock 详解](./附录/C_GTConvBlock详解.md) - GTConvBlock 的结构、各子模块、V1 的改进
