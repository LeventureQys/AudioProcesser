# 4.3 è½»é‡åŒ–æŠ€æœ¯ç»¼è¿° - å››å¤§æŠ€æœ¯è·¯å¾„è¯¦è§£

## ğŸ¯ æ¦‚è¿°ï¼šè½»é‡åŒ–çš„å·¥å…·ç®±

åœ¨ç†è§£äº†ä¸ºä»€ä¹ˆéœ€è¦è½»é‡åŒ–ä»¥åŠå¦‚ä½•åˆ†æå¤æ‚åº¦ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦å…·ä½“çš„**æŠ€æœ¯å·¥å…·**æ¥å®ç°è½»é‡åŒ–ã€‚æœ¬èŠ‚å°†ç³»ç»Ÿä»‹ç»å››å¤§è½»é‡åŒ–æŠ€æœ¯è·¯å¾„ï¼Œä¸ºæ¨¡å‹æ•ˆç‡ä¼˜åŒ–æä¾›å®Œæ•´çš„æ–¹æ³•è®ºã€‚

---

## ğŸ”§ æŠ€æœ¯å…¨æ™¯ï¼šå››å¤§è½»é‡åŒ–è·¯å¾„

è½»é‡åŒ–æŠ€æœ¯å¯ä»¥å½’çº³ä¸ºå››å¤§è·¯å¾„ï¼Œå„æœ‰ä¾§é‡ï¼Œå¯ä»¥ç»„åˆä½¿ç”¨ï¼š

```
è½»é‡åŒ–æŠ€æœ¯ä½“ç³»
    â”œâ”€ ç½‘ç»œå‰ªæ(Pruning)ï¼šç§»é™¤å†—ä½™
    â”œâ”€ çŸ¥è¯†è’¸é¦(Distillation)ï¼šå¤§æ•™å°
    â”œâ”€ é‡åŒ–(Quantization)ï¼šé™ä½ç²¾åº¦  
    â””â”€ é«˜æ•ˆæ¶æ„è®¾è®¡ï¼šæºå¤´ä¼˜åŒ–
```

æ¯ç§æŠ€æœ¯è§£å†³ä¸åŒçš„é—®é¢˜ï¼Œé€‚ç”¨äºä¸åŒçš„åœºæ™¯ï¼Œéœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©å’Œç»„åˆã€‚

---

## âœ‚ï¸ 1. ç½‘ç»œå‰ªæ(Pruning)ï¼šç§»é™¤å†—ä½™è¿æ¥

### æ ¸å¿ƒæ€æƒ³ï¼šç¨€ç–åŒ–ä¸é‡è¦æ€§è¯„ä¼°
> "ç¥ç»ç½‘ç»œé€šå¸¸æœ‰è¿‡å¤šçš„å‚æ•°ï¼Œå…¶ä¸­å¾ˆå¤šæ˜¯å†—ä½™çš„æˆ–ä¸é‡è¦çš„ã€‚é€šè¿‡è¯†åˆ«å¹¶ç§»é™¤è¿™äº›å†—ä½™è¿æ¥ï¼Œå¯ä»¥å¤§å¹…å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚"

### å‰ªæçš„ä¸‰ç§ç±»å‹
#### (1) éç»“æ„åŒ–å‰ªæ (Unstructured Pruning)
**ç‰¹ç‚¹**ï¼šç§»é™¤å•ä¸ªæƒé‡ï¼Œäº§ç”Ÿç¨€ç–çŸ©é˜µ
```python
# éç»“æ„åŒ–å‰ªæç¤ºä¾‹
def unstructured_pruning(weight_matrix, sparsity=0.5):
    """ç§»é™¤æƒé‡çŸ©é˜µä¸­æœ€ä¸é‡è¦çš„50%å…ƒç´ """
    # è®¡ç®—é‡è¦æ€§ï¼ˆç»å¯¹å€¼å¤§å°ï¼‰
    importance = torch.abs(weight_matrix)
    
    # æ‰¾åˆ°é˜ˆå€¼
    threshold = torch.quantile(importance, sparsity)
    
    # åˆ›å»ºæ©ç 
    mask = importance > threshold
    
    # åº”ç”¨å‰ªæ
    pruned_weight = weight_matrix * mask
    
    return pruned_weight, mask
```

**ä¼˜ç‚¹**ï¼šçµæ´»æ€§é«˜ï¼Œå‹ç¼©æ•ˆæœå¥½
**ç¼ºç‚¹**ï¼šéœ€è¦ç¨€ç–è®¡ç®—æ”¯æŒï¼Œå®é™…åŠ é€Ÿæœ‰é™

#### (2) ç»“æ„åŒ–å‰ªæ (Structured Pruning)
**ç‰¹ç‚¹**ï¼šç§»é™¤æ•´ä¸ªé€šé“æˆ–æ»¤æ³¢å™¨
```python
def structured_pruning(conv_layer, pruning_ratio=0.3):
    """ç§»é™¤å·ç§¯å±‚æœ€ä¸é‡è¦çš„30%é€šé“"""
    # è®¡ç®—æ¯ä¸ªæ»¤æ³¢å™¨çš„é‡è¦æ€§ï¼ˆå¦‚L1èŒƒæ•°ï¼‰
    filter_importance = torch.norm(conv_layer.weight, p=1, dim=(1,2,3))
    
    # æ’åºå¹¶é€‰æ‹©è¦ä¿ç•™çš„æ»¤æ³¢å™¨
    num_to_keep = int(conv_layer.out_channels * (1 - pruning_ratio))
    _, indices = torch.topk(filter_importance, num_to_keep)
    
    # åˆ›å»ºæ–°å±‚
    new_conv = nn.Conv2d(
        conv_layer.in_channels,
        num_to_keep,
        conv_layer.kernel_size,
        conv_layer.stride,
        conv_layer.padding
    )
    
    # å¤åˆ¶ä¿ç•™çš„æƒé‡
    new_conv.weight.data = conv_layer.weight.data[indices]
    new_conv.bias.data = conv_layer.bias.data[indices]
    
    return new_conv
```

**ä¼˜ç‚¹**ï¼šå®é™…åŠ é€Ÿæ˜æ˜¾ï¼Œä¸éœ€è¦ç‰¹æ®Šç¡¬ä»¶
**ç¼ºç‚¹**ï¼šçµæ´»æ€§è¾ƒä½ï¼Œå¯èƒ½æŸå¤±æ›´å¤šæ€§èƒ½

#### (3) åŸºäºé‡è¦æ€§çš„å‰ªæ (Importance-based Pruning)
æ ¹æ®ä¸åŒçš„é‡è¦æ€§å‡†åˆ™ï¼š

| é‡è¦æ€§å‡†åˆ™ | è®¡ç®—æ–¹æ³• | é€‚ç”¨åœºæ™¯ |
|------------|----------|----------|
| **ç»å¯¹å€¼å¤§å°** | $|w|$ | ç®€å•å¿«é€Ÿ |
| **æ¢¯åº¦ä¿¡æ¯** | $\left|\frac{\partial L}{\partial w}\right|$ | è€ƒè™‘ä»»åŠ¡ç›¸å…³æ€§ |
| **Hessianä¿¡æ¯** | $w^2 \cdot H_{ii}$ | ç†è®ºæœ€ä¼˜ä½†è®¡ç®—å¤æ‚ |
| **æ¿€æ´»å€¼ç»Ÿè®¡** | è¾“å‡ºæ¿€æ´»å€¼çš„ç»Ÿè®¡é‡ | è€ƒè™‘å®é™…ä½¿ç”¨æƒ…å†µ |

### å‰ªæçš„å·¥ä½œæµç¨‹
```
é¢„è®­ç»ƒæ¨¡å‹ â†’ é‡è¦æ€§è¯„ä¼° â†’ å‰ªææ“ä½œ â†’ å¾®è°ƒæ¢å¤ â†’ è¯„ä¼°éªŒè¯
    â†“          â†“          â†“         â†“         â†“
åŸºå‡†æ€§èƒ½   è¯†åˆ«å†—ä½™    ç§»é™¤è¿æ¥  æ¢å¤æ€§èƒ½  ç¡®è®¤æ•ˆæœ
```

### å‰ªæç­–ç•¥é€‰æ‹©
| ç­–ç•¥ç±»å‹ | å‰ªæç²’åº¦ | å†è®­ç»ƒéœ€æ±‚ | ç¡¬ä»¶è¦æ±‚ | é€‚ç”¨åœºæ™¯ |
|----------|----------|------------|----------|----------|
| **ä¸€æ¬¡æ€§å‰ªæ** | ä¸€æ¬¡æ€§ç§»é™¤ | éœ€è¦å¾®è°ƒ | é€šç”¨ç¡¬ä»¶ | ç®€å•åº”ç”¨ |
| **è¿­ä»£å‰ªæ** | é€æ­¥ç§»é™¤ | å¤šæ¬¡å¾®è°ƒ | é€šç”¨ç¡¬ä»¶ | é«˜å‹ç¼©æ¯” |
| **åŠ¨æ€å‰ªæ** | è¿è¡Œæ—¶å†³å®š | è®­ç»ƒæ—¶é›†æˆ | ç‰¹æ®Šç¡¬ä»¶ | è‡ªé€‚åº”ç³»ç»Ÿ |

### å‰ªæçš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ
#### æŒ‘æˆ˜1ï¼šæ€§èƒ½æ¢å¤å›°éš¾
**è§£å†³æ–¹æ¡ˆ**ï¼š
- **æ¸è¿›å¼å‰ªæ**ï¼šé€æ­¥å‰ªæï¼Œæ¯æ¬¡å‰ªæåå¾®è°ƒ
- **çŸ¥è¯†è’¸é¦è¾…åŠ©**ï¼šç”¨åŸæ¨¡å‹æŒ‡å¯¼å‰ªæåæ¨¡å‹
- **è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼šå¾®è°ƒæ—¶ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡

#### æŒ‘æˆ˜2ï¼šå®é™…åŠ é€Ÿæœ‰é™
**è§£å†³æ–¹æ¡ˆ**ï¼š
- **ç»“æ„åŒ–å‰ªæ**ï¼šäº§ç”Ÿè§„æ•´çš„ç»“æ„ä¾¿äºåŠ é€Ÿ
- **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šä½¿ç”¨æ”¯æŒç¨€ç–è®¡ç®—çš„ç¡¬ä»¶
- **è½¯ä»¶ä¼˜åŒ–**ï¼šä¼˜åŒ–ç¨€ç–çŸ©é˜µè¿ç®—åº“

#### æŒ‘æˆ˜3ï¼šè¿‡å‰ªæé—®é¢˜
**è§£å†³æ–¹æ¡ˆ**ï¼š
- **æ—©æœŸåœæ­¢**ï¼šç›‘æ§éªŒè¯é›†æ€§èƒ½
- **æ•æ„Ÿæ€§åˆ†æ**ï¼šåˆ†æå„å±‚å¯¹å‰ªæçš„æ•æ„Ÿåº¦
- **å¼¹æ€§å‰ªæ**ï¼šä¿ç•™å‰ªæçš„çµæ´»æ€§

### è¯­éŸ³é™å™ªä¸­çš„å‰ªæåº”ç”¨
åœ¨è¯­éŸ³é™å™ªä¸­ï¼Œå‰ªæç‰¹åˆ«æœ‰æ•ˆï¼Œå› ä¸ºï¼š
1. **ä»»åŠ¡ç›¸å¯¹ç®€å•**ï¼šç›¸æ¯”å›¾åƒåˆ†ç±»ï¼Œè¯­éŸ³é™å™ªå†—ä½™æ›´å¤š
2. **å®æ—¶æ€§è¦æ±‚**ï¼šéœ€è¦æè‡´çš„æ•ˆç‡
3. **æ¨¡å‹ç›¸å¯¹è¾ƒå°**ï¼šå®¹æ˜“è¿›è¡Œå‰ªæåˆ†æ

**å…¸å‹å‹ç¼©æ•ˆæœ**ï¼šå¯å‡å°‘50-90%å‚æ•°ï¼Œæ€§èƒ½æŸå¤±1-5%

---

## ğŸ“ 2. çŸ¥è¯†è’¸é¦(Knowledge Distillation)ï¼šå¤§æ•™å°

### æ ¸å¿ƒæ€æƒ³ï¼šæ•™å¸ˆ-å­¦ç”ŸèŒƒå¼
> "ç”¨ä¸€ä¸ªå¤§å‹ã€é«˜æ€§èƒ½çš„æ•™å¸ˆæ¨¡å‹(Teacher)æŒ‡å¯¼ä¸€ä¸ªå°å‹ã€é«˜æ•ˆçš„å­¦ç”Ÿæ¨¡å‹(Student)è®­ç»ƒï¼Œä½¿å­¦ç”Ÿæ¨¡å‹èƒ½æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºï¼Œè¾¾åˆ°æ¥è¿‘æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚"

### è’¸é¦çš„ä¸‰ç§å½¢å¼
#### (1) å“åº”è’¸é¦ (Response Distillation)
**æ€æƒ³**ï¼šè®©å­¦ç”Ÿè¾“å‡ºåŒ¹é…æ•™å¸ˆè¾“å‡º
```python
def response_distillation(student_logits, teacher_logits, temperature=3.0):
    """å“åº”è’¸é¦æŸå¤±"""
    # ä½¿ç”¨æ¸©åº¦è½¯åŒ–æ¦‚ç‡åˆ†å¸ƒ
    soft_student = F.softmax(student_logits / temperature, dim=-1)
    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)
    
    # KLæ•£åº¦æŸå¤±
    kld_loss = F.kl_div(
        soft_student.log(), 
        soft_teacher, 
        reduction='batchmean'
    )
    
    return kld_loss * (temperature ** 2)  # ç¼©æ”¾å›åŸå°ºåº¦
```

#### (2) ç‰¹å¾è’¸é¦ (Feature Distillation)
**æ€æƒ³**ï¼šè®©å­¦ç”Ÿä¸­é—´ç‰¹å¾åŒ¹é…æ•™å¸ˆç‰¹å¾
```python
def feature_distillation(student_feats, teacher_feats):
    """ç‰¹å¾è’¸é¦æŸå¤±"""
    # å¯èƒ½éœ€è¦å¯¹ç‰¹å¾è¿›è¡Œé€‚é…ï¼ˆå¦‚ä¸åŒç»´åº¦ï¼‰
    if student_feats.shape != teacher_feats.shape:
        # ä½¿ç”¨é€‚é…å±‚
        adapter = nn.Linear(student_feats.shape[1], teacher_feats.shape[1])
        student_feats = adapter(student_feats)
    
    # ç‰¹å¾ç›¸ä¼¼åº¦æŸå¤±ï¼ˆå¦‚MSEæˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    loss = F.mse_loss(student_feats, teacher_feats)
    
    return loss
```

#### (3) å…³ç³»è’¸é¦ (Relation Distillation)
**æ€æƒ³**ï¼šè®©å­¦ç”Ÿæ ·æœ¬é—´å…³ç³»åŒ¹é…æ•™å¸ˆå…³ç³»
```python
def relation_distillation(student_feats, teacher_feats):
    """å…³ç³»è’¸é¦æŸå¤±"""
    # è®¡ç®—æ ·æœ¬é—´å…³ç³»ï¼ˆå¦‚GramçŸ©é˜µï¼‰
    student_gram = torch.mm(student_feats, student_feats.t())
    teacher_gram = torch.mm(teacher_feats, teacher_feats.t())
    
    # å…³ç³»åŒ¹é…æŸå¤±
    loss = F.mse_loss(student_gram, teacher_gram)
    
    return loss
```

### è’¸é¦æ¸©åº¦(Temperature)çš„ä½œç”¨
**æ¸©åº¦å‚æ•°** $T$ æ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„"è½¯åŒ–"ç¨‹åº¦ï¼š
- **$T=1$**ï¼šåŸå§‹softmaxï¼Œæ¦‚ç‡åˆ†å¸ƒå°–é”
- **$T>1$**ï¼šè½¯åŒ–åˆ†å¸ƒï¼Œæ­ç¤ºç±»åˆ«é—´å…³ç³»
- **$T \to \infty$**ï¼šå‡åŒ€åˆ†å¸ƒï¼Œæ‰€æœ‰ç±»åˆ«æ¦‚ç‡ç›¸ç­‰

```python
def softmax_with_temperature(logits, temperature):
    """å¸¦æ¸©åº¦çš„softmax"""
    return F.softmax(logits / temperature, dim=-1)
```

### è’¸é¦æŸå¤±å‡½æ•°è®¾è®¡
é€šå¸¸ç»“åˆå¤šç§æŸå¤±ï¼š
```python
def distillation_loss(student_output, teacher_output, 
                     ground_truth, alpha=0.5, temperature=3.0):
    """å®Œæ•´çš„è’¸é¦æŸå¤±"""
    # 1. ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆå­¦ç”ŸvsçœŸå®æ ‡ç­¾ï¼‰
    hard_loss = F.cross_entropy(student_output, ground_truth)
    
    # 2. è½¯æ ‡ç­¾æŸå¤±ï¼ˆå­¦ç”Ÿvsæ•™å¸ˆï¼‰
    soft_loss = response_distillation(
        student_output, teacher_output, temperature
    )
    
    # 3. ç»„åˆæŸå¤±
    total_loss = alpha * soft_loss + (1 - alpha) * hard_loss
    
    return total_loss
```

### è’¸é¦åœ¨è¯­éŸ³é™å™ªä¸­çš„åº”ç”¨
è¯­éŸ³é™å™ªè’¸é¦çš„ç‰¹æ®Šæ€§ï¼š
1. **å›å½’ä»»åŠ¡**ï¼šè¾“å‡ºæ˜¯è¿ç»­å€¼ï¼Œä¸æ˜¯åˆ†ç±»æ¦‚ç‡
2. **å¤šç›®æ ‡ä¼˜åŒ–**ï¼šåŒæ—¶ä¼˜åŒ–å¤šä¸ªè´¨é‡æŒ‡æ ‡
3. **æ—¶åºç‰¹æ€§**ï¼šéœ€è¦è€ƒè™‘æ—¶é—´è¿ç»­æ€§

**é€‚åº”æ–¹æ¡ˆ**ï¼š
```python
def speech_enhancement_distillation(student_output, teacher_output,
                                   clean_target, alpha=0.7):
    """è¯­éŸ³å¢å¼ºè’¸é¦æŸå¤±"""
    # 1. ç›´æ¥å›å½’æŸå¤±ï¼ˆå­¦ç”Ÿvså¹²å‡€è¯­éŸ³ï¼‰
    direct_loss = F.mse_loss(student_output, clean_target)
    
    # 2. è’¸é¦æŸå¤±ï¼ˆå­¦ç”Ÿvsæ•™å¸ˆï¼‰
    # å¯¹äºå›å½’ä»»åŠ¡ï¼Œå¯ä»¥ç›´æ¥ç”¨MSEæˆ–æ›´å¤æ‚çš„ç›¸ä¼¼åº¦
    distill_loss = F.mse_loss(student_output, teacher_output)
    
    # 3. æ„ŸçŸ¥æŸå¤±ï¼ˆå¯é€‰ï¼‰
    # ä½¿ç”¨é¢„è®­ç»ƒçš„æ„ŸçŸ¥æ¨¡å‹è¯„ä¼°ç›¸ä¼¼åº¦
    
    # ç»„åˆæŸå¤±
    total_loss = alpha * distill_loss + (1 - alpha) * direct_loss
    
    return total_loss
```

### è’¸é¦çš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜
#### ä¼˜åŠ¿
1. **æ€§èƒ½æå‡**ï¼šå­¦ç”Ÿæ¨¡å‹å¯èƒ½è¶…è¿‡åŒç­‰å¤§å°çš„ç›´æ¥è®­ç»ƒæ¨¡å‹
2. **çµæ´»æ€§**ï¼šå¯ä»¥è’¸é¦ä¸åŒæ¶æ„çš„æ•™å¸ˆæ¨¡å‹
3. **çŸ¥è¯†ä¼ é€’**ï¼šä¼ é€’æ•™å¸ˆå­¦åˆ°çš„"æš—çŸ¥è¯†"(dark knowledge)

#### æŒ‘æˆ˜
1. **æ•™å¸ˆæ¨¡å‹è´¨é‡**ï¼šéœ€è¦é«˜è´¨é‡æ•™å¸ˆæ¨¡å‹
2. **è®­ç»ƒå¤æ‚åº¦**ï¼šéœ€è¦è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
3. **è¶…å‚æ•°è°ƒä¼˜**ï¼šæ¸©åº¦ã€æƒé‡ç­‰éœ€è¦ä»”ç»†è°ƒæ•´

---

## ğŸ›ï¸ 3. é‡åŒ–(Quantization)ï¼šé™ä½æ•°å€¼ç²¾åº¦

### æ ¸å¿ƒæ€æƒ³ï¼šç”¨ä½ç²¾åº¦è¡¨ç¤ºé«˜ç²¾åº¦
> "ç¥ç»ç½‘ç»œè®¡ç®—é€šå¸¸ä½¿ç”¨32ä½æµ®ç‚¹æ•°(float32)ï¼Œä½†å¾ˆå¤šåº”ç”¨åœºæ™¯å¯ä»¥ç”¨æ›´ä½ç²¾åº¦ï¼ˆå¦‚16ä½ã€8ä½ç”šè‡³1ä½ï¼‰è¡¨ç¤ºï¼Œä»è€Œå¤§å¹…å‡å°‘å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚"

### é‡åŒ–çº§åˆ«
| ç²¾åº¦çº§åˆ« | æ¯”ç‰¹æ•° | åŠ¨æ€èŒƒå›´ | å…¸å‹åº”ç”¨ |
|----------|--------|----------|----------|
| **FP32** | 32 bits | ~10Â³â¸ | è®­ç»ƒã€é«˜ç²¾åº¦æ¨ç† |
| **FP16** | 16 bits | ~10âµ | è®­ç»ƒã€æ··åˆç²¾åº¦æ¨ç† |
| **INT8** | 8 bits | 256çº§åˆ« | ç§»åŠ¨ç«¯æ¨ç† |
| **INT4** | 4 bits | 16çº§åˆ« | æç«¯èµ„æºå—é™ |
| **Binary** | 1 bit | 2çº§åˆ« | ç ”ç©¶ã€ç‰¹æ®Šç¡¬ä»¶ |

### é‡åŒ–æ–¹æ³•åˆ†ç±»
#### (1) è®­ç»ƒåé‡åŒ– (Post-Training Quantization, PTQ)
**æµç¨‹**ï¼šå…ˆè®­ç»ƒFP32æ¨¡å‹ï¼Œå†é‡åŒ–
```python
def post_training_quantization(model, calibration_data):
    """è®­ç»ƒåé‡åŒ–"""
    # 1. æ ¡å‡†ï¼šæ”¶é›†æ¿€æ´»å€¼ç»Ÿè®¡ä¿¡æ¯
    model.eval()
    with torch.no_grad():
        for data in calibration_data:
            _ = model(data)
            # æ”¶é›†æ¯å±‚æ¿€æ´»å€¼çš„min/maxæˆ–ç›´æ–¹å›¾
    
    # 2. é‡åŒ–ï¼šæ ¹æ®ç»Ÿè®¡ä¿¡æ¯ç¡®å®šé‡åŒ–å‚æ•°
    quantized_model = quantize_model(model, calibration_stats)
    
    # 3. è¯„ä¼°é‡åŒ–åæ€§èƒ½
    evaluate_quantized_model(quantized_model)
    
    return quantized_model
```

**ä¼˜ç‚¹**ï¼šç®€å•å¿«é€Ÿï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒ
**ç¼ºç‚¹**ï¼šå¯èƒ½æ€§èƒ½ä¸‹é™è¾ƒå¤š

#### (2) é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization-Aware Training, QAT)
**æµç¨‹**ï¼šåœ¨è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ
```python
def quantization_aware_training(model):
    """é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ"""
    # 1. åœ¨æ¨¡å‹ä¸­æ’å…¥é‡åŒ–/åé‡åŒ–èŠ‚ç‚¹
    qat_model = insert_fake_quant_nodes(model)
    
    # 2. ç”¨æ¨¡æ‹Ÿé‡åŒ–è®­ç»ƒ
    for epoch in range(num_epochs):
        for data, target in dataloader:
            # å‰å‘ä¼ æ’­ï¼ˆåŒ…å«æ¨¡æ‹Ÿé‡åŒ–ï¼‰
            output = qat_model(data)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­ï¼ˆç›´é€šä¼°è®¡å™¨STEï¼‰
            loss.backward()
            optimizer.step()
    
    # 3. å¯¼å‡ºçœŸæ­£é‡åŒ–æ¨¡å‹
    final_quantized = export_quantized_model(qat_model)
    
    return final_quantized
```

**ä¼˜ç‚¹**ï¼šæ€§èƒ½æŸå¤±å°
**ç¼ºç‚¹**ï¼šè®­ç»ƒå¤æ‚ï¼Œéœ€è¦æ›´å¤šæ—¶é—´

#### (3) åŠ¨æ€é‡åŒ– (Dynamic Quantization)
**ç‰¹ç‚¹**ï¼šè¿è¡Œæ—¶åŠ¨æ€ç¡®å®šé‡åŒ–å‚æ•°
```python
def dynamic_quantization(model):
    """åŠ¨æ€é‡åŒ–"""
    # æƒé‡é™æ€é‡åŒ–
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {nn.Linear, nn.Conv2d},  # è¦é‡åŒ–çš„æ¨¡å—ç±»å‹
        dtype=torch.qint8        # é‡åŒ–æ•°æ®ç±»å‹
    )
    
    return quantized_model
```

**ä¼˜ç‚¹**ï¼šçµæ´»ï¼Œé€‚åº”ä¸åŒè¾“å…¥
**ç¼ºç‚¹**ï¼šè¿è¡Œæ—¶å¼€é”€

### é‡åŒ–ä¸­çš„å…³é”®æŠ€æœ¯
#### ç›´é€šä¼°è®¡å™¨ (Straight-Through Estimator, STE)
è§£å†³é‡åŒ–ä¸å¯å¯¼é—®é¢˜ï¼š
```python
class FakeQuantizeSTE(torch.autograd.Function):
    """æ¨¡æ‹Ÿé‡åŒ–çš„STEå®ç°"""
    @staticmethod
    def forward(ctx, input, scale, zero_point):
        # å‰å‘ï¼šé‡åŒ–å†åé‡åŒ–
        quantized = torch.round(input / scale + zero_point)
        dequantized = (quantized - zero_point) * scale
        return dequantized
    
    @staticmethod
    def backward(ctx, grad_output):
        # åå‘ï¼šç›´é€šæ¢¯åº¦
        return grad_output, None, None
```

#### å¯¹ç§° vs. éå¯¹ç§°é‡åŒ–
| ç±»å‹ | å…¬å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| **å¯¹ç§°é‡åŒ–** | $x_q = \text{clip}(\lfloor \frac{x}{s} \rceil, -2^{b-1}, 2^{b-1}-1)$ | å®ç°ç®€å•ï¼Œé›¶ç‚¹æ˜¯0 | ä¸èƒ½å……åˆ†åˆ©ç”¨åŠ¨æ€èŒƒå›´ |
| **éå¯¹ç§°é‡åŒ–** | $x_q = \text{clip}(\lfloor \frac{x - z}{s} \rceil, 0, 2^b-1)$ | æ›´å¥½åˆ©ç”¨åŠ¨æ€èŒƒå›´ | å®ç°å¤æ‚ï¼Œéœ€è¦é›¶ç‚¹ |

#### é€å±‚ vs. é€é€šé“é‡åŒ–
- **é€å±‚é‡åŒ–**ï¼šæ¯å±‚ç”¨åŒä¸€ç»„é‡åŒ–å‚æ•°
- **é€é€šé“é‡åŒ–**ï¼šæ¯ä¸ªé€šé“ç”¨ä¸åŒé‡åŒ–å‚æ•°ï¼ˆæ•ˆæœæ›´å¥½ä½†æ›´å¤æ‚ï¼‰

### é‡åŒ–åœ¨è¯­éŸ³é™å™ªä¸­çš„ç‰¹æ®Šæ€§
è¯­éŸ³é™å™ªå¯¹é‡åŒ–çš„ç‰¹æ®Šè¦æ±‚ï¼š
1. **åŠ¨æ€èŒƒå›´å¤§**ï¼šè¯­éŸ³ä¿¡å·åŠ¨æ€èŒƒå›´å¯è¾¾60dBä»¥ä¸Š
2. **ç²¾åº¦æ•æ„Ÿ**ï¼šç›¸ä½ä¿¡æ¯å¯¹é‡åŒ–æ•æ„Ÿ
3. **å®æ—¶æ€§è¦æ±‚**ï¼šé‡åŒ–ä¸èƒ½å¢åŠ å¤ªå¤šå»¶è¿Ÿ

**é€‚åº”ç­–ç•¥**ï¼š
- **æ··åˆç²¾åº¦**ï¼šå…³é”®å±‚ç”¨é«˜ç²¾åº¦ï¼Œå…¶ä»–å±‚ç”¨ä½ç²¾åº¦
- **ç›¸ä½ç‰¹æ®Šå¤„ç†**ï¼šç›¸ä½ç”¨æ›´é«˜ç²¾åº¦æˆ–ç‰¹æ®Šè¡¨ç¤º
- **æ„ŸçŸ¥ä¼˜åŒ–**ï¼šä¼˜åŒ–å¯¹äººè€³é‡è¦çš„é¢‘æ®µ

### é‡åŒ–çš„ç¡¬ä»¶æ”¯æŒ
ä¸åŒç¡¬ä»¶å¯¹é‡åŒ–çš„æ”¯æŒç¨‹åº¦ï¼š
| ç¡¬ä»¶å¹³å° | INT8æ”¯æŒ | INT4æ”¯æŒ | äºŒè¿›åˆ¶æ”¯æŒ | ç‰¹æ®ŠæŒ‡ä»¤ |
|----------|----------|----------|------------|----------|
| **ARM CPU** | ä¸€èˆ¬ | æœ‰é™ | æ—  | éƒ¨åˆ†æœ‰DSPæ‰©å±• |
| **ç§»åŠ¨GPU** | è¾ƒå¥½ | æœ‰é™ | æ—  | Tensoræ ¸å¿ƒ |
| **ä¸“ç”¨NPU** | ä¼˜ç§€ | å¥½ | å¯èƒ½æœ‰ | ä¸“ç”¨é‡åŒ–æŒ‡ä»¤ |
| **FPGA** | å¯å®šåˆ¶ | å¯å®šåˆ¶ | å¯å®šåˆ¶ | å®Œå…¨å¯å®šåˆ¶ |

---

## ğŸ—ï¸ 4. é«˜æ•ˆæ¶æ„è®¾è®¡ï¼šä»æºå¤´å‡å°‘è®¡ç®—é‡

### æ ¸å¿ƒæ€æƒ³ï¼šè®¾è®¡æ—¶å°±è€ƒè™‘æ•ˆç‡
> "ä¸å…¶å…ˆè®¾è®¡å¤§æ¨¡å‹å†å‹ç¼©ï¼Œä¸å¦‚ä»ä¸€å¼€å§‹å°±è®¾è®¡é«˜æ•ˆçš„æ¶æ„ï¼Œä»æ ¹æœ¬ä¸Šå‡å°‘è®¡ç®—éœ€æ±‚ã€‚"

### é«˜æ•ˆæ¶æ„è®¾è®¡åŸåˆ™
#### åŸåˆ™1ï¼šæ·±åº¦å¯åˆ†ç¦»å·ç§¯ (Depthwise Separable Convolution)
**æ€æƒ³**ï¼šå°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºæ·±åº¦å·ç§¯+é€ç‚¹å·ç§¯

```python
class DepthwiseSeparableConv(nn.Module):
    """æ·±åº¦å¯åˆ†ç¦»å·ç§¯"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        
        # 1. æ·±åº¦å·ç§¯ï¼ˆæ¯ä¸ªé€šé“ç‹¬ç«‹ï¼‰
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size,
            groups=in_channels,  # å…³é”®ï¼šåˆ†ç»„æ•°=è¾“å…¥é€šé“æ•°
            padding=kernel_size//2
        )
        
        # 2. é€ç‚¹å·ç§¯ï¼ˆ1Ã—1å·ç§¯æ··åˆé€šé“ï¼‰
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)
    
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
```

**è®¡ç®—é‡å¯¹æ¯”**ï¼š
- **æ ‡å‡†å·ç§¯**ï¼š$K^2 \times C_{in} \times C_{out} \times H \times W$
- **æ·±åº¦å¯åˆ†ç¦»**ï¼š$(K^2 \times C_{in} + C_{in} \times C_{out}) \times H \times W$

**å‹ç¼©æ¯”**ï¼šçº¦ $K^2$ å€ï¼ˆå¦‚3Ã—3å·ç§¯çº¦9å€ï¼‰

#### åŸåˆ™2ï¼šç“¶é¢ˆç»“æ„ (Bottleneck Design)
**æ€æƒ³**ï¼šå…ˆç”¨1Ã—1å·ç§¯é™ç»´ï¼Œè¿›è¡Œä¸»è¦è®¡ç®—ï¼Œå†å‡ç»´

```python
class BottleneckBlock(nn.Module):
    """ç“¶é¢ˆå—"""
    def __init__(self, in_channels, bottleneck_channels, out_channels):
        super().__init__()
        
        # 1. é™ç»´
        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, 1)
        
        # 2. ä¸»è¦è®¡ç®—ï¼ˆåœ¨ä½ç»´åº¦ï¼‰
        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, 3, padding=1)
        
        # 3. å‡ç»´
        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, 1)
        
    def forward(self, x):
        identity = x
        
        x = self.conv1(x)
        x = F.relu(x)
        
        x = self.conv2(x)
        x = F.relu(x)
        
        x = self.conv3(x)
        
        # æ®‹å·®è¿æ¥
        x += identity
        x = F.relu(x)
        
        return x
```

**ä¼˜åŠ¿**ï¼šåœ¨ä½ç»´åº¦è¿›è¡Œè®¡ç®—ï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡

#### åŸåˆ™3ï¼šåˆ†ç»„å·ç§¯ (Grouped Convolution)
**æ€æƒ³**ï¼šå°†é€šé“åˆ†ç»„ï¼Œæ¯ç»„ç‹¬ç«‹å·ç§¯

```python
class GroupedConv(nn.Module):
    """åˆ†ç»„å·ç§¯"""
    def __init__(self, in_channels, out_channels, kernel_size, groups=4):
        super().__init__()
        
        # æ¯ç»„å¤„ç†çš„é€šé“æ•°
        assert in_channels % groups == 0
        assert out_channels % groups == 0
        
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size,
            groups=groups,  # å…³é”®å‚æ•°
            padding=kernel_size//2
        )
    
    def forward(self, x):
        return self.conv(x)
```

**è®¡ç®—é‡å‡å°‘**ï¼š$groups$ å€
**æƒè¡¡**ï¼šç»„é—´ä¿¡æ¯äº¤äº’å‡å°‘ï¼Œéœ€è¦ä»”ç»†è®¾è®¡åˆ†ç»„æ•°

#### åŸåˆ™4ï¼šæ³¨æ„åŠ›å¼•å¯¼çš„ç¨€ç–æ€§
**æ€æƒ³**ï¼šåªè®¡ç®—é‡è¦çš„åŒºåŸŸ

```python
class SparseAttentionConv(nn.Module):
    """æ³¨æ„åŠ›å¼•å¯¼çš„ç¨€ç–å·ç§¯"""
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__()
        
        # 1. æ³¨æ„åŠ›æ¨¡å—ï¼ˆå†³å®šå“ªäº›ä½ç½®é‡è¦ï¼‰
        self.attention = nn.Sequential(
            nn.Conv2d(in_channels, in_channels//4, 1),
            nn.ReLU(),
            nn.Conv2d(in_channels//4, 1, 1),
            nn.Sigmoid()  # è¾“å‡º0-1çš„é‡è¦æ€§åˆ†æ•°
        )
        
        # 2. ç¨€ç–å·ç§¯ï¼ˆåªè®¡ç®—é‡è¦åŒºåŸŸï¼‰
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
    
    def forward(self, x):
        # è®¡ç®—æ³¨æ„åŠ›æ©ç 
        attention_mask = self.attention(x)
        
        # åº”ç”¨ç¨€ç–è®¡ç®—ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„ç¨€ç–è®¡ç®—
        weighted_x = x * attention_mask
        
        # å·ç§¯
        output = self.conv(weighted_x)
        
        return output
```

### é«˜æ•ˆæ¶æ„è®¾è®¡æ¨¡å¼
#### æ¨¡å¼1ï¼šMobileNetç³»åˆ—
**æ ¸å¿ƒ**ï¼šæ·±åº¦å¯åˆ†ç¦»å·ç§¯ + çº¿æ€§ç“¶é¢ˆ + å€’æ®‹å·®

#### æ¨¡å¼2ï¼šShuffleNetç³»åˆ—
**æ ¸å¿ƒ**ï¼šåˆ†ç»„å·ç§¯ + é€šé“æ´—ç‰Œ

#### æ¨¡å¼3ï¼šEfficientNetç³»åˆ—
**æ ¸å¿ƒ**ï¼šå¤åˆç¼©æ”¾ï¼ˆåŒæ—¶ç¼©æ”¾æ·±åº¦ã€å®½åº¦ã€åˆ†è¾¨ç‡ï¼‰

#### æ¨¡å¼4ï¼šGhostNetç³»åˆ—
**æ ¸å¿ƒ**ï¼šç”¨å»‰ä»·æ“ä½œç”Ÿæˆ"å¹½çµ"ç‰¹å¾å›¾

### è¯­éŸ³é™å™ªçš„é«˜æ•ˆæ¶æ„è®¾è®¡
è¯­éŸ³é™å™ªçš„ç‰¹æ®Šè€ƒè™‘ï¼š
1. **æ—¶é¢‘ç‰¹æ€§**ï¼šæ—¶é—´å’Œé¢‘ç‡ç»´åº¦æ€§è´¨ä¸åŒ
2. **å› æœæ€§è¦æ±‚**ï¼šå®æ—¶å¤„ç†éœ€è¦å› æœè®¾è®¡
3. **å¬è§‰ç‰¹æ€§**ï¼šéœ€è¦è€ƒè™‘äººè€³æ„ŸçŸ¥ç‰¹æ€§

**GTCRNçš„è®¾è®¡ä½“ç°**ï¼š
- **ERBé¢‘å¸¦åˆ†ç»„**ï¼šç¬¦åˆå¬è§‰ç‰¹æ€§ï¼Œå‡å°‘è®¡ç®—
- **åˆ†ç»„å·ç§¯**ï¼šå‡å°‘è®¡ç®—é‡ï¼Œå¹³è¡¡ä¿¡æ¯äº¤äº’
- **å› æœè†¨èƒ€å·ç§¯**ï¼šä¿è¯å®æ—¶æ€§ï¼Œæ‰©å¤§æ„Ÿå—é‡
- **è½»é‡GRU**ï¼šæ—¶åºå»ºæ¨¡ï¼Œæ§åˆ¶å‚æ•°é‡

---

## ğŸ”„ æŠ€æœ¯ç»„åˆä¸é€‰æ‹©ç­–ç•¥

### 1. æŠ€æœ¯ç»„åˆçš„ååŒæ•ˆåº”
ä¸åŒæŠ€æœ¯å¯ä»¥**ç»„åˆä½¿ç”¨**ï¼Œäº§ç”ŸååŒæ•ˆåº”ï¼š

#### å…¸å‹ç»„åˆæµç¨‹
```
é«˜æ•ˆæ¶æ„è®¾è®¡ï¼ˆæºå¤´ä¼˜åŒ–ï¼‰
    â†“
çŸ¥è¯†è’¸é¦ï¼ˆæ€§èƒ½æå‡ï¼‰
    â†“
ç½‘ç»œå‰ªæï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰
    â†“
é‡åŒ–ï¼ˆæœ€ç»ˆéƒ¨ç½²ä¼˜åŒ–ï¼‰
```

#### ç»„åˆç¤ºä¾‹ï¼šå‰ªæ+é‡åŒ–
```python
def prune_and_quantize(model, pruning_sparsity=0.5, quant_bits=8):
    """å…ˆå‰ªæå†é‡åŒ–"""
    # 1. å‰ªæ
    pruned_model = prune_model(model, sparsity=pruning_sparsity)
    
    # 2. å¾®è°ƒæ¢å¤æ€§èƒ½
    fine_tune(pruned_model)
    
    # 3. é‡åŒ–
    quantized_model = quantize_model(pruned_model, bits=quant_bits)
    
    return quantized_model
```

### 2. æŠ€æœ¯é€‰æ‹©å†³ç­–æ ‘
```python
def select_lightweight_techniques(constraints):
    """æ ¹æ®çº¦æŸé€‰æ‹©è½»é‡åŒ–æŠ€æœ¯"""
    techniques = []
    
    # æ ¹æ®å‚æ•°é‡çº¦æŸ
    if constraints["max_params"] < 100000:  # <100K
        techniques.append("efficient_architecture")
        techniques.append("aggressive_pruning")
    
    # æ ¹æ®è®¡ç®—é‡çº¦æŸ
    if constraints["max_macs"] < 100000000:  # <100MMACs
        techniques.append("quantization_int8")
        techniques.append("depthwise_separable")
    
    # æ ¹æ®å®æ—¶æ€§è¦æ±‚
    if constraints["max_latency"] < 20:  # <20ms
        techniques.append("causal_design")
        techniques.append("structured_pruning")
    
    # æ ¹æ®è®­ç»ƒèµ„æº
    if constraints["training_resources"] == "limited":
        techniques.append("post_training_quantization")
        # é¿å…éœ€è¦å¤§é‡è®­ç»ƒçš„è’¸é¦
    
    return techniques
```

### 3. æŠ€æœ¯å®æ–½çš„é˜¶æ®µè€ƒè™‘
| å¼€å‘é˜¶æ®µ | é€‚ç”¨æŠ€æœ¯ | åŸå›  |
|----------|----------|------|
| **è®¾è®¡é˜¶æ®µ** | é«˜æ•ˆæ¶æ„è®¾è®¡ | ä»æºå¤´ä¼˜åŒ–æœ€æœ‰æ•ˆ |
| **è®­ç»ƒé˜¶æ®µ** | çŸ¥è¯†è’¸é¦ã€QAT | éœ€è¦è®­ç»ƒè¿‡ç¨‹å‚ä¸ |
| **åå¤„ç†é˜¶æ®µ** | PTQã€å‰ªæ | è®­ç»ƒååº”ç”¨ï¼Œçµæ´» |
| **éƒ¨ç½²é˜¶æ®µ** | ç¡¬ä»¶ç‰¹å®šä¼˜åŒ– | é’ˆå¯¹ç›®æ ‡ç¡¬ä»¶ä¼˜åŒ– |

### 4. ä¸åŒåœºæ™¯çš„æŠ€æœ¯æ¨è
| åº”ç”¨åœºæ™¯ | ä¼˜å…ˆçº§æŠ€æœ¯ | æ¬¡è¦æŠ€æœ¯ | é¿å…æŠ€æœ¯ |
|----------|------------|----------|----------|
| **å®æ—¶é€šä¿¡** | é«˜æ•ˆæ¶æ„ã€å› æœè®¾è®¡ | PTQã€ç»“æ„åŒ–å‰ªæ | éå› æœè®¾è®¡ã€é«˜ç²¾åº¦ |
| **ç§»åŠ¨å½•éŸ³** | çŸ¥è¯†è’¸é¦ã€QAT | éç»“æ„åŒ–å‰ªæ | è¿‡åº¦é‡åŒ– |
| **IoTè®¾å¤‡** | é«˜æ•ˆæ¶æ„ã€ä½æ¯”ç‰¹é‡åŒ– | æç«¯å‰ªæ | å¤æ‚è’¸é¦ |
| **ç ”ç©¶æ¢ç´¢** | æ–°æ¶æ„è®¾è®¡ | å„ç§æŠ€æœ¯å®éªŒ | æ— é™åˆ¶ |

---

## ğŸ“Š æ€§èƒ½-æ•ˆç‡æƒè¡¡åˆ†æ

### 1. æŠ€æœ¯å¯¹æ€§èƒ½çš„å½±å“
| æŠ€æœ¯ | é€šå¸¸æ€§èƒ½æŸå¤± | å¤‡æ³¨ |
|------|--------------|------|
| **å‰ªæï¼ˆé€‚åº¦ï¼‰** | 1-5% | è¿‡åº¦å‰ªææŸå¤±æ›´å¤§ |
| **è’¸é¦ï¼ˆå¥½æ•™å¸ˆï¼‰** | å¯èƒ½æå‡ | å­¦ç”Ÿå¯èƒ½è¶…è¿‡åŒç­‰å¤§å°ç›´æ¥è®­ç»ƒæ¨¡å‹ |
| **é‡åŒ–ï¼ˆINT8ï¼‰** | 1-3% | æ›´ä½ç²¾åº¦æŸå¤±æ›´å¤§ |
| **é«˜æ•ˆæ¶æ„** | 0-10% | è®¾è®¡å¥½çš„æ¶æ„æŸå¤±å° |

### 2. æŠ€æœ¯å¯¹æ•ˆç‡çš„æå‡
| æŠ€æœ¯ | å‚æ•°é‡å‡å°‘ | è®¡ç®—é‡å‡å°‘ | å†…å­˜å‡å°‘ |
|------|------------|------------|----------|
| **å‰ªæï¼ˆ50%ï¼‰** | 50% | 30-50% | 50% |
| **é‡åŒ–ï¼ˆINT8ï¼‰** | 75% | 2-4å€åŠ é€Ÿ | 75% |
| **æ·±åº¦å¯åˆ†ç¦»** | å˜åŒ– | 5-10å€ | å˜åŒ– |
| **åˆ†ç»„å·ç§¯** | å˜åŒ– | groupså€ | å˜åŒ– |

### 3. å®é™…éƒ¨ç½²çš„ç»¼åˆæ•ˆæœ
å…¸å‹è½»é‡åŒ–æµç¨‹çš„æ•ˆæœï¼š
```
åŸå§‹æ¨¡å‹ï¼šå‚æ•°é‡10Mï¼Œè®¡ç®—é‡1GMACsï¼Œå»¶è¿Ÿ50ms
    â†“ é«˜æ•ˆæ¶æ„è®¾è®¡
é˜¶æ®µ1ï¼šå‚æ•°é‡5Mï¼Œè®¡ç®—é‡500MMACsï¼Œå»¶è¿Ÿ30msï¼ˆæŸå¤±2%ï¼‰
    â†“ çŸ¥è¯†è’¸é¦
é˜¶æ®µ2ï¼šå‚æ•°é‡5Mï¼Œè®¡ç®—é‡500MMACsï¼Œå»¶è¿Ÿ30msï¼ˆæ¢å¤ç”šè‡³æå‡ï¼‰
    â†“ å‰ªæï¼ˆ50%ï¼‰
é˜¶æ®µ3ï¼šå‚æ•°é‡2.5Mï¼Œè®¡ç®—é‡300MMACsï¼Œå»¶è¿Ÿ20msï¼ˆæŸå¤±3%ï¼‰
    â†“ é‡åŒ–ï¼ˆINT8ï¼‰
æœ€ç»ˆï¼šå‚æ•°é‡0.625Mï¼Œè®¡ç®—é‡150MMACsï¼Œå»¶è¿Ÿ10msï¼ˆæŸå¤±5%ï¼‰

æ€»æ•ˆæœï¼šå‚æ•°é‡å‡å°‘16å€ï¼Œè®¡ç®—é‡å‡å°‘6.7å€ï¼Œå»¶è¿Ÿå‡å°‘5å€ï¼Œæ€§èƒ½æŸå¤±5%
```

---

## ğŸ’¡ å…³é”®æ´å¯Ÿï¼šè½»é‡åŒ–æŠ€æœ¯çš„è‰ºæœ¯

### 1. æ²¡æœ‰é“¶å¼¹ï¼Œåªæœ‰åˆé€‚ç»„åˆ
æ¯ä¸ªæŠ€æœ¯éƒ½æœ‰**é€‚ç”¨åœºæ™¯å’Œå±€é™æ€§**ï¼Œéœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©å’Œç»„åˆã€‚

### 2. æ—©æœŸè®¾è®¡çš„é‡è¦æ€§
**é«˜æ•ˆæ¶æ„è®¾è®¡**æ˜¯æœ€æ ¹æœ¬çš„ä¼˜åŒ–ï¼ŒåæœŸä¼˜åŒ–éš¾ä»¥å¼¥è¡¥æ—©æœŸè®¾è®¡çš„ä½æ•ˆã€‚

### 3. ç«¯åˆ°ç«¯ä¼˜åŒ–æ€ç»´
è½»é‡åŒ–éœ€è¦**ç®—æ³•ã€è½¯ä»¶ã€ç¡¬ä»¶ååŒä¼˜åŒ–**ï¼Œå•ä¸€ç¯èŠ‚ä¼˜åŒ–æ•ˆæœæœ‰é™ã€‚

### 4. æ•°æ®é©±åŠ¨çš„å†³ç­–
é€šè¿‡**å®éªŒå’Œæµ‹é‡**é€‰æ‹©æŠ€æœ¯ï¼Œè€Œä¸æ˜¯åŸºäºç›´è§‰æˆ–ç»éªŒã€‚

### 5. æŒç»­è¿­ä»£çš„è¿‡ç¨‹
è½»é‡åŒ–æ˜¯**æŒç»­ä¼˜åŒ–**çš„è¿‡ç¨‹ï¼Œéšç€æŠ€æœ¯å‘å±•å’Œéœ€æ±‚å˜åŒ–éœ€è¦ä¸æ–­è°ƒæ•´ã€‚

---

## ğŸ§ª å®è·µå»ºè®®ï¼šå¼€å§‹ä½ çš„è½»é‡åŒ–ä¹‹æ—…

### èµ·æ­¥å»ºè®®
1. **å»ºç«‹åŸºå‡†**ï¼šå…ˆæœ‰ä¸€ä¸ªå¯å·¥ä½œçš„åŸºçº¿æ¨¡å‹
2. **åˆ†æç“¶é¢ˆ**ï¼šä½¿ç”¨å¤æ‚åº¦åˆ†æè¯†åˆ«ä¸»è¦ç“¶é¢ˆ
3. **é€‰æ‹©æŠ€æœ¯**ï¼šæ ¹æ®ç“¶é¢ˆå’Œçº¦æŸé€‰æ‹©1-2ç§æŠ€æœ¯
4. **å®éªŒéªŒè¯**ï¼šå°è§„æ¨¡å®éªŒéªŒè¯æ•ˆæœ
5. **é€æ­¥æ‰©å±•**ï¼šæˆåŠŸåå†å°è¯•æ›´å¤æ‚çš„æŠ€æœ¯ç»„åˆ

### å®éªŒè®¾è®¡æ¡†æ¶
```python
def lightweight_experiment_framework(baseline_model, constraints):
    """è½»é‡åŒ–å®éªŒæ¡†æ¶"""
    experiments = []
    
    # æŠ€æœ¯é€‰é¡¹
    techniques = ["pruning", "distillation", "quantization", "efficient_arch"]
    
    # å•æŠ€æœ¯å®éªŒ
    for tech in techniques:
        experiment = {
            "technique": tech,
            "model": apply_technique(baseline_model, tech),
            "metrics": evaluate_with_constraints(baseline_model, constraints)
        }
        experiments.append(experiment)
    
    # ç»„åˆæŠ€æœ¯å®éªŒï¼ˆé€‰æ‹©æ•ˆæœå¥½çš„ç»„åˆï¼‰
    # ...
    
    # åˆ†æç»“æœï¼Œé€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
    best_experiment = select_best(experiments, constraints)
    
    return best_experiment
```

### æŒç»­å­¦ä¹ ä¸æ›´æ–°
è½»é‡åŒ–æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œéœ€è¦ï¼š
1. **è·Ÿè¸ªç ”ç©¶**ï¼šå…³æ³¨æœ€æ–°è®ºæ–‡å’ŒæŠ€æœ¯
2. **å®è·µéªŒè¯**ï¼šæ–°æŠ€æœ¯éœ€è¦å®é™…éªŒè¯
3. **ç¤¾åŒºå‚ä¸**ï¼šå‚ä¸å¼€æºç¤¾åŒºå’ŒæŠ€æœ¯è®¨è®º
4. **è·¨é¢†åŸŸå­¦ä¹ **ï¼šä»å…¶ä»–é¢†åŸŸå€Ÿé‰´æ€æƒ³

---

## ğŸ“š å­¦ä¹ è¦ç‚¹æ€»ç»“

### å¿…é¡»æŒæ¡çš„æŠ€æœ¯
1. **å››å¤§æŠ€æœ¯è·¯å¾„**ï¼šå‰ªæã€è’¸é¦ã€é‡åŒ–ã€é«˜æ•ˆæ¶æ„
2. **æ¯ç§æŠ€æœ¯çš„å˜ä½“**ï¼šå¦‚ç»“æ„åŒ–vséç»“æ„åŒ–å‰ªæ
3. **æŠ€æœ¯ç»„åˆæ–¹æ³•**ï¼šå¦‚ä½•ååŒä½¿ç”¨å¤šç§æŠ€æœ¯
4. **é€‰æ‹©å†³ç­–ä¾æ®**ï¼šæ ¹æ®çº¦æŸé€‰æ‹©åˆé€‚æŠ€æœ¯

### æ€ç»´è®­ç»ƒé‡ç‚¹
1. **ç³»ç»Ÿæ€ç»´**ï¼šè€ƒè™‘æŠ€æœ¯é—´çš„ç›¸äº’ä½œç”¨
2. **æƒè¡¡æ€ç»´**ï¼šåœ¨æ€§èƒ½æŸå¤±å’Œæ•ˆç‡æå‡é—´å¹³è¡¡
3. **å®éªŒæ€ç»´**ï¼šé€šè¿‡å®éªŒéªŒè¯æŠ€æœ¯æ•ˆæœ
4. **è¿­ä»£æ€ç»´**ï¼šæŒç»­ä¼˜åŒ–å’Œæ”¹è¿›

### å®è·µæŠ€èƒ½åŸ¹å…»
1. å®ç°åŸºæœ¬çš„å‰ªæã€è’¸é¦ã€é‡åŒ–æŠ€æœ¯
2. è®¾è®¡é«˜æ•ˆæ¶æ„å¹¶è¯„ä¼°æ•ˆæœ
3. ç»„åˆå¤šç§æŠ€æœ¯è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–
4. æ ¹æ®å®é™…çº¦æŸé€‰æ‹©å’Œè°ƒæ•´æŠ€æœ¯æ–¹æ¡ˆ

---

## ğŸ æœ¬èŠ‚æ€»ç»“ï¼šè½»é‡åŒ–æŠ€æœ¯çš„å·¥å…·ç®±

### æŠ€æœ¯ä½“ç³»çš„ä»·å€¼
å››å¤§æŠ€æœ¯è·¯å¾„æä¾›äº†**å®Œæ•´çš„å·¥å…·ç®±**ï¼š
- **å‰ªæ**ï¼šåšå‡æ³•ï¼Œç§»é™¤å†—ä½™
- **è’¸é¦**ï¼šåšè½¬åŒ–ï¼Œä¼ é€’çŸ¥è¯†
- **é‡åŒ–**ï¼šåšå‹ç¼©ï¼Œé™ä½ç²¾åº¦
- **é«˜æ•ˆæ¶æ„**ï¼šåšè®¾è®¡ï¼Œæºå¤´ä¼˜åŒ–

### å†å²è§†è§’
è½»é‡åŒ–æŠ€æœ¯çš„å‘å±•åæ˜ äº†**å·¥ç¨‹æ€ç»´çš„æˆç†Ÿ**ï¼š
```
æ—©æœŸï¼šå•ä¸€æŠ€æœ¯æ¢ç´¢
    â†“
ä¸­æœŸï¼šæŠ€æœ¯ç»„åˆå°è¯•
    â†“
ç°åœ¨ï¼šç³»ç»ŸåŒ–æ–¹æ³•è®º
    â†“
æœªæ¥ï¼šè‡ªåŠ¨åŒ–è½»é‡åŒ–
```

### å­¦ä¹ æ–¹æ³•å¯ç¤º
1. **ç†è®ºä¸å®è·µç»“åˆ**ï¼šæ—¢ç†è§£åŸç†ä¹ŸæŒæ¡å®ç°
2. **å¹¿åº¦ä¸æ·±åº¦å¹³è¡¡**ï¼šæ—¢äº†è§£å…¨æ™¯ä¹Ÿæ·±å…¥å…³é”®æŠ€æœ¯
3. **ç‹¬ç«‹ä¸ç»„åˆå¹¶é‡**ï¼šæ—¢æŒæ¡å•æŠ€æœ¯ä¹Ÿå­¦ä¹ ç»„åˆ
4. **å­¦ä¹ ä¸å®è·µå¾ªç¯**ï¼šå­¦ä¹ åå®è·µï¼Œå®è·µä¸­å­¦ä¹ 

> **å…³é”®æ´å¯Ÿ**ï¼šè½»é‡åŒ–æŠ€æœ¯å‘Šè¯‰æˆ‘ä»¬ï¼Œä¼˜åŒ–ä¸ä»…æ˜¯**æŠ€æœ¯åº”ç”¨**ï¼Œæ›´æ˜¯**ç³»ç»Ÿæ€ç»´**ã€‚ç†è§£å„ç§æŠ€æœ¯çš„åŸç†ã€é€‚ç”¨åœºæ™¯å’Œç›¸äº’ä½œç”¨ï¼Œæ‰èƒ½åšå‡ºæ˜æ™ºçš„å·¥ç¨‹å†³ç­–ã€‚

---

### å­¦ä¹ è·¯å¾„æé†’
```
ç†è§£è½»é‡åŒ–å¿…è¦æ€§ï¼ˆ4.1ï¼‰
    â†“
æŒæ¡å¤æ‚åº¦åˆ†ææ–¹æ³•ï¼ˆ4.2ï¼‰
    â†“
å­¦ä¹ è½»é‡åŒ–æŠ€æœ¯å·¥å…·ç®±ï¼ˆæœ¬èŠ‚ï¼‰
    â†“
åˆ†æå®é™…æ¡ˆä¾‹ï¼ˆ4.4-4.5ï¼‰
    â†“
å½¢æˆå®Œæ•´è½»é‡åŒ–æ–¹æ³•è®ºï¼ˆç¬¬å››ç« æ€»ç»“ï¼‰
```

---

*ä¸‹ä¸€èŠ‚é¢„å‘Šï¼šæˆ‘ä»¬å°†æ·±å…¥åˆ†æ**4.4 RNNoiseï¼šè½»é‡åŒ–çš„å…ˆé©±**ï¼Œçœ‹çœ‹è¿™äº›æŠ€æœ¯å¦‚ä½•åœ¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„è½»é‡åŒ–è¯­éŸ³é™å™ªç³»ç»Ÿä¸­åº”ç”¨ã€‚*
