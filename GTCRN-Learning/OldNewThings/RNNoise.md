# 4.4 RNNoise：轻量化的先驱

## 概述

RNNoise是2017年Mozilla的Jean-Marc Valin搞出来的一个语音降噪系统。这哥们同时也是Opus编解码器的作者，所以对音频处理这块非常熟。

说实话，RNNoise在当时算是个挺大的突破。在它之前，要么用传统方法（谱减法、维纳滤波），效果一般但跑得快；要么用深度学习，效果好但根本没法实时跑。RNNoise第一次证明了：用很小的模型（才100K参数）也能做出比传统方法好的降噪效果，而且还能实时。

现在RNNoise已经被集成到很多地方了：WebRTC、Discord、OBS这些都在用。

## 当时面临的问题

### 传统方法不够用了

**谱减法**的思路很简单：先在没人说话的时候估计一下噪声长啥样，然后从带噪语音里把噪声减掉：

$$|\hat{S}(\omega)|^2 = |Y(\omega)|^2 - \alpha|\hat{N}(\omega)|^2$$

但问题很明显：
- 噪声得是平稳的，不然估不准
- 减完之后频谱上会出现一些孤立的峰，听起来像"音乐噪声"，很难受
- 相位完全没处理

**维纳滤波**理论上更漂亮：

$$H(\omega) = \frac{P_S(\omega)}{P_S(\omega) + P_N(\omega)}$$

但实际用起来，语音功率谱$P_S$和噪声功率谱$P_N$都得估计，估不准效果就差。

### 深度学习太重了

2017年那会儿，深度学习在语音增强上的尝试基本都是几百万参数的大模型，根本没法在手机上实时跑。而且很多用的是双向RNN，需要看到未来的帧才能处理当前帧，延迟太大。

实时通信对延迟很敏感，一般要求40ms以内，不然打电话会觉得很奇怪。

### 为什么搞混合架构？

RNNoise的思路是：传统信号处理和深度学习各有优势，干脆结合起来。

- 传统方法提取特征：物理意义明确，计算量小
- 神经网络做决策：学习能力强，能处理复杂情况

这样神经网络的输入维度就从几百维降到了65维，参数量自然就小了。

## 整体架构

整个流程大概是这样：

```
原始音频
    ↓
传统信号处理前端
    ├─ 32频带分析（ERB尺度）
    ├─ 基音估计
    ├─ 谐波相关性计算
    └─ DCT变换
    ↓
65维特征向量
    ↓
3层GRU（每层256单元）
    ↓
32维增益 + 1维VAD
    ↓
应用增益 + 逆变换
    ↓
增强音频
```

### 为什么不直接用STFT频谱？

你可能会问：干嘛搞这么复杂，直接把STFT频谱扔进网络不行吗？

问题是960点FFT会产生480个复数频点，也就是960维实数输入。这样网络参数量会爆炸。而且相邻频点高度相关，信息冗余很大。

RNNoise的65维特征把输入维度降了15倍，同时保留了关键信息。

## 特征提取细节

### 基本参数

- 采样率：48kHz（内部统一用这个，方便和WebRTC对接）
- 帧长：20ms（960样本）
- 帧移：10ms（480样本，50%重叠）
- FFT点数：960

频率分辨率是48000/960=50Hz，对于语音来说够用了。

### 32频带分析

RNNoise用32个ERB频带来分析频谱。ERB（Equivalent Rectangular Bandwidth）是基于人耳听觉特性的频带划分——低频分辨率高，高频分辨率低，和人耳的感知特性一致。

ERB带宽公式：
$$B_{ERB}(f_c) = 24.7 \times (4.37 \times 10^{-3} f_c + 1)$$

具体来说：
- 100Hz处，带宽约25Hz
- 1000Hz处，带宽约130Hz
- 10000Hz处，带宽约680Hz

每个频带的能量：
$$E_b = \sum_{k \in \mathcal{K}_b} |X(k)|^2$$

### 65维特征是怎么来的

**前32维：频谱能量的DCT**

先算对数能量：
$$L_b = \log_{10}(10^{-2} + E_b)$$

加$10^{-2}$是为了数值稳定，不然能量为0的时候log会炸。

然后做DCT变换。为什么要DCT？因为相邻频带的能量高度相关（语音频谱是平滑的），DCT可以去相关，把能量集中到前几个系数。这和MFCC的思路差不多。

**中间32维：谐波相关性的DCT**

谐波相关性衡量的是每个频带和基音谐波的相似程度：
$$C_b = \frac{\text{Re}\left(\sum_{k \in \mathcal{K}_b} X(k) P^*(k)\right)}{\sqrt{E_b \cdot E_p}}$$

$C_b$接近1说明这个频带主要是语音谐波，接近0说明是噪声。这个特征对区分语音和噪声很有用，因为语音有明显的谐波结构，噪声没有。

同样做DCT变换。

**最后1维：基音周期**

$$F_{65} = 0.01 \times (p - 300)$$

$p$是基音周期（样本数），300样本对应160Hz，大概是人声基频的中心。这样特征值大概在[-2, 2]范围内。

### 这65维特征在干嘛

简单说：
- 前32维：频谱长什么样
- 中间32维：哪些是语音谐波
- 最后1维：基音大概多高

网络学的就是从这65维特征到32个频带增益的映射。

## GRU网络

### 为什么用GRU不用LSTM

GRU比LSTM少一个门，参数量大概是LSTM的75%。在语音降噪这个任务上，两者性能差不多，但GRU更快。

2017年Transformer刚出来，还没人用在语音上。就算现在，Transformer做实时语音处理也有点麻烦，因为自注意力需要看到整个序列。

### GRU的数学

更新门：$z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)$

重置门：$r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r)$

候选状态：$\tilde{h}_t = \tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h)$

最终状态：$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$

直观理解：$z_t$控制保留多少历史，$r_t$控制遗忘多少历史。

### 网络结构

```
65维输入
    ↓
Conv1D (128) → Conv1D (256)
    ↓
GRU (256) → GRU (256) → GRU (256)
    ↓
拼接所有层输出 (1024维)
    ↓
全连接 → 33维输出 (32增益 + 1 VAD)
    ↓
Sigmoid
```

有个细节：RNNoise把所有GRU层的输出都拼起来了，而不是只用最后一层。这样不同层捕获的不同时间尺度的信息都能用上，有点像残差连接的效果。

### 因果性

所有GRU都是单向的，只看历史不看未来。这样才能实时处理，延迟只有帧长（20ms）加上处理时间（不到1ms）。

### 参数量

压缩前大概1.36M参数，压缩后约100K。压缩用了稀疏化、量化、剪枝这些技术。

## 后处理

网络输出的增益不能直接用，需要做一些后处理。

### 时间平滑

$$s_t(b) = 0.3 \cdot g_t(b) + 0.7 \cdot s_{t-1}(b)$$

这是个一阶低通滤波器，防止增益在相邻帧之间跳变太大，不然会有"咔嗒"声。

### 过减

$$g'_t(b) = \max(1 - 1.5 \times (1 - s_t(b)), 0.01)$$

过减是个经典技巧：宁可多减一点，也不要留残余噪声。系数1.5意味着如果网络说保留80%，实际只保留70%。

0.01是谱下限，保证不会完全静音，不然听起来会很奇怪（"空洞感"）。

### VAD引导

$$g''_t(b) = \begin{cases}
(g'_t(b))^{0.8}, & p_{\text{vad}} > 0.7 \\
(g'_t(b))^{1.2}, & p_{\text{vad}} < 0.3 \\
g'_t(b), & \text{otherwise}
\end{cases}$$

VAD概率高说明是语音，保守一点少减；VAD概率低说明是噪声，激进一点多减。

### 频带到频点的映射

32个频带增益要插值到480个线性频点上，用的是线性插值。

### 重建

$$\hat{X}(k) = G(k) |X(k)| e^{j\angle X(k)}$$

注意相位直接用的带噪相位，没有处理。这是RNNoise的一个局限：在低信噪比下，带噪相位和干净相位差很多，会影响音质。

## 性能

### 计算效率

RTF（实时因子）= 处理时间 / 音频时长

RNNoise在普通CPU上RTF约0.07，也就是处理1秒音频只要0.07秒。在树莓派4上也能跑到0.15，完全满足实时要求。

### 降噪效果

| 方法 | PESQ | STOI |
|------|------|------|
| 谱减法 | 2.15 | 0.82 |
| 维纳滤波 | 2.35 | 0.85 |
| RNNoise | 2.54 | 0.92 |

PESQ 2.5以上一般认为是可接受的质量。RNNoise比传统方法好了一截，虽然比后来的DCCRN（2.85）、FullSubNet（2.92）还有差距，但人家参数量是RNNoise的几十倍。

### 不同噪声的表现

办公室噪声、键盘敲击这种效果比较好；街道噪声、咖啡厅噪声这种复杂场景效果一般。

## 局限性

1. **频率分辨率有限**：32个频带对于窄带噪声（比如50Hz电源哼声）可能不够精细
2. **模型容量有限**：100K参数学不了太复杂的模式，极低信噪比下效果下降明显
3. **不处理相位**：低信噪比下会有"浑浊"感
4. **只能处理单说话人**：多人说话或者会议场景不行
5. **泛化能力一般**：对没见过的噪声类型适应性有限

## 影响

RNNoise的意义不只是这个具体的算法，更重要的是它证明了几件事：

1. 小模型也能超越传统方法
2. 混合架构是可行的
3. 频带级别处理够用了，不需要频点级别

这些思路影响了后来很多工作，包括DeepFilterNet、GTCRN等。

另外RNNoise完全开源（BSD-3），代码写得也很清楚，对后来的研究者帮助很大。

## 后续发展

- **PercepNet (2020)**：同一个作者的改进版，加了感知加权损失
- **DeepFilterNet (2022)**：继承了ERB频带思想，加了深度滤波处理相位
- 各种工业界的定制版本

到现在RNNoise还在被广泛使用，特别是在资源受限的场景下。对于学习语音增强来说，RNNoise是个很好的起点——架构清晰，代码完整，文档详细。
