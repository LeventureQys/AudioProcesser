# GTCRN 算法学习提纲
## —— 从工程与机器学习视角理解语音降噪算法的设计思路

---

## 前言：学习目标

通过本提纲的学习，你将能够：
1. 理解语音降噪问题的数学本质
2. 掌握从传统算法到深度学习的演进逻辑
3. 学会分析"计算效率 vs 降噪性能"的工程权衡
4. 理解GTCRN每个模块的设计动机
5. 具备独立设计轻量级降噪网络的思维框架

---

## 第一章：语音降噪的基本假设与问题建模

### 1.1 信号模型的数学表达
- 加性噪声模型：`y(t) = s(t) + n(t)`
- 时域 vs 频域表示的选择
- 短时傅里叶变换(STFT)：为什么它是语音处理的基石？

### 1.2 降噪问题的本质
- 信号分离问题的不适定性(ill-posed problem)
- 为什么需要先验假设？
- 语音信号的统计特性：
  - 短时平稳性
  - 稀疏性
  - 谐波结构

### 1.3 评价指标体系
- 信噪比(SNR)与分段信噪比(SegSNR)
- PESQ/POLQA：感知质量评估
- STOI：可懂度评估
- SI-SNR：尺度不变信噪比
- 工程指标：RTF(实时因子)、参数量、MACs

---

## 第二章：传统降噪算法——理解问题的起点

### 2.1 谱减法(Spectral Subtraction)
- 核心思想：噪声谱估计 + 直接相减
- 数学推导与实现
- 局限性：音乐噪声(musical noise)的产生

### 2.2 维纳滤波(Wiener Filter)
- 最小均方误差(MMSE)准则
- 先验SNR与后验SNR
- 决策导向(Decision-Directed)方法
- 局限性：对非平稳噪声的适应性差

### 2.3 统计模型方法
- MMSE-STSA估计器
- Log-MMSE估计器
- 噪声估计算法：最小统计量法(Minimum Statistics)

### 2.4 传统算法的共同困境
- **假设过强**：依赖噪声平稳性假设
- **泛化能力差**：参数需要针对特定场景调优
- **非线性建模能力弱**：难以处理复杂噪声环境
- **语音失真**：过度抑制导致的语音损伤

---

## 第三章：深度学习破局——从数据驱动到端到端

### 3.1 为什么深度学习能解决传统方法的困境？
- 从"手工设计特征"到"自动学习特征"
- 非线性建模能力的本质优势
- 大规模数据的价值：学习噪声的多样性

### 3.2 早期尝试：DNN作为映射函数
- 理想二值掩码(IBM)与理想比值掩码(IRM)
- 频谱映射(Spectral Mapping)
- 全连接网络的局限性：参数量爆炸、缺乏时序建模

### 3.3 卷积神经网络(CNN)的引入
- 局部感受野与参数共享
- 时频域的2D卷积 vs 时域的1D卷积
- 代表工作：SEGAN、Wave-U-Net

### 3.4 循环神经网络(RNN)的时序建模
- LSTM/GRU：长短期依赖的捕获
- 因果性约束：实时处理的必要条件
- 代表工作：DTLN、RNNoise

### 3.5 编码器-解码器(Encoder-Decoder)架构
- U-Net结构的引入
- 跳跃连接(Skip Connection)的作用
- 代表工作：Conv-TasNet、DCCRN

---

## 第四章：轻量化的工程挑战——效率与性能的博弈

### 4.1 为什么需要轻量化？
- 边缘设备的计算约束
- 实时性要求：延迟 < 40ms
- 功耗限制：移动设备、IoT场景

### 4.2 计算复杂度分析
- 参数量(Parameters)：模型大小
- MACs/FLOPs：计算量
- 内存占用：激活值存储
- 延迟(Latency)：端到端处理时间

### 4.3 轻量化技术综述
- **网络剪枝(Pruning)**：移除冗余连接
- **知识蒸馏(Knowledge Distillation)**：大模型指导小模型
- **量化(Quantization)**：降低数值精度
- **高效架构设计**：从源头减少计算量

### 4.4 RNNoise：轻量化的先驱
- 混合信号处理 + 神经网络
- GRU的高效实现
- Pitch滤波器的引入
- 局限性：性能天花板、频带处理粗糙

### 4.5 DeepFilterNet：频域深度滤波
- ERB频带分组的思想
- 深度滤波(Deep Filtering)的概念
- 多阶段处理架构
- 计算量与性能的权衡

---

## 第五章：GTCRN的设计哲学——每个模块的工程动机

### 5.1 整体架构概览
- 编码器-解码器主干
- 时序建模模块
- 频带处理策略
- 参数量：48.2K，MACs：33.0M/s

### 5.2 输入表示的选择：为什么用ERB？
- 人耳听觉的非线性特性
- ERB(等效矩形带宽)的物理意义
- 频带压缩带来的计算节省
- 与Mel频谱的对比

### 5.3 分组卷积(Grouped Convolution)的妙用
- 标准卷积的计算量分析
- 分组卷积的数学原理
- 通道间信息交互的权衡
- 为什么选择特定的分组数？

### 5.4 时序卷积(Temporal Convolution)的设计
- 因果卷积保证实时性
- 膨胀卷积(Dilated Convolution)扩大感受野
- 与RNN的对比：并行性 vs 长程依赖

### 5.5 GRU模块的角色
- 为什么还需要RNN？
- GRU vs LSTM的选择
- 隐藏层维度的设计考量
- 单向 vs 双向的权衡

### 5.6 跳跃连接与特征融合
- 编码器特征的保留
- 不同层级特征的语义差异
- 融合方式的选择：拼接 vs 相加

### 5.7 输出层设计：掩码 vs 直接映射
- 复数掩码(Complex Mask)的优势
- 相位信息的处理
- 掩码范围的约束

---

## 第六章：训练策略与损失函数设计

### 6.1 数据准备
- 语音数据集：VCTK、DNS Challenge
- 噪声数据集：DEMAND、AudioSet
- 数据增强策略
- 动态混合(Dynamic Mixing)

### 6.2 损失函数的选择
- 时域损失：SI-SNR Loss
- 频域损失：Magnitude Loss、Phase Loss
- 感知损失：PESQ Loss
- 多任务损失的组合

### 6.3 训练技巧
- 学习率调度
- 梯度裁剪
- 批量大小的影响
- 课程学习(Curriculum Learning)

---

## 第七章：从GTCRN学到的设计方法论

### 7.1 问题驱动的设计思维
- 明确约束条件：计算量、延迟、性能
- 分析瓶颈：哪个模块最耗资源？
- 针对性优化：用最小代价解决最大问题

### 7.2 模块化设计原则
- 每个模块解决一个子问题
- 模块间的接口设计
- 可替换性与可扩展性

### 7.3 借鉴与创新的平衡
- 站在巨人肩膀上：复用成熟的设计
- 针对性改进：根据具体问题调整
- 实验验证：消融实验的重要性

### 7.4 工程实现的考量
- 算子友好性：避免碎片化操作
- 内存访问模式：连续访问 vs 随机访问
- 部署框架的限制：ONNX、TensorRT兼容性

---

## 第八章：动手实践——构建你自己的降噪模型

### 8.1 实验环境搭建
- PyTorch环境配置
- 数据集准备
- 评估工具链

### 8.2 从零实现GTCRN
- 逐模块实现与测试
- 前向传播的调试
- 参数量与计算量验证

### 8.3 消融实验设计
- 分组数的影响
- GRU层数的影响
- ERB频带数的影响

### 8.4 模型优化与部署
- ONNX导出
- 量化实验
- 实时流式推理实现

---

## 附录

### A. 数学基础回顾
- 复数运算与傅里叶变换
- 卷积的数学定义
- 循环神经网络的梯度流

### B. 代码资源
- GTCRN官方仓库：https://github.com/Xiaobin-Rong/gtcrn
- 相关论文列表
- 推荐的开源实现

### C. 扩展阅读
- 语音增强综述论文
- 轻量化网络设计综述
- 实时音频处理系统设计

---

## 学习路线建议

```
第1周：第一章 + 第二章
       ↓ 理解问题本质，动手实现谱减法和维纳滤波

第2周：第三章
       ↓ 阅读经典论文，理解深度学习方法的演进

第3周：第四章
       ↓ 分析RNNoise和DeepFilterNet的代码

第4周：第五章 + 第六章
       ↓ 精读GTCRN论文，理解每个设计决策

第5周：第七章 + 第八章
       ↓ 动手实现，进行消融实验
```

---

*提纲版本：v1.0*
*创建日期：2025-01-24*
*适用对象：具备DSP和机器学习基础的学习者*
